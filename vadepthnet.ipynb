{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2540835,"sourceType":"datasetVersion","datasetId":1540599},{"sourceId":2957421,"sourceType":"datasetVersion","datasetId":1813091},{"sourceId":6929416,"sourceType":"datasetVersion","datasetId":3834525},{"sourceId":6941125,"sourceType":"datasetVersion","datasetId":3410500},{"sourceId":7041542,"sourceType":"datasetVersion","datasetId":4030312},{"sourceId":7057960,"sourceType":"datasetVersion","datasetId":4055379}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **准备阶段**","metadata":{}},{"cell_type":"markdown","source":"---\nGitHub克隆复制文件","metadata":{}},{"cell_type":"code","source":"# 克隆GitHub模型\n# 切换工作空间\n%cd /kaggle/working\n# 克隆文件夹\n!git clone https://github.com/ykzou9/VADepthNet.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 复制单个文件\n!wget https://raw.githubusercontent.com/ykzou9/VADepthNet/main/data_splits/vad_nyu2_test.txt","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 复制单个文件\n!wget https://raw.githubusercontent.com/ykzou9/VADepthNet/main/vadepthnet/test.py","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n切换工作空间","metadata":{}},{"cell_type":"code","source":"# 切换工作空间\n%cd /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:04:28.759739Z","iopub.execute_input":"2023-11-28T16:04:28.760560Z","iopub.status.idle":"2023-11-28T16:04:28.766921Z","shell.execute_reply.started":"2023-11-28T16:04:28.760525Z","shell.execute_reply":"2023-11-28T16:04:28.765869Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 切换运行路径\n%cd /kaggle/working/VADepthNet/vadepthnet","metadata":{"execution":{"iopub.status.busy":"2023-11-28T02:14:42.380735Z","iopub.execute_input":"2023-11-28T02:14:42.381099Z","iopub.status.idle":"2023-11-28T02:14:42.387372Z","shell.execute_reply.started":"2023-11-28T02:14:42.381068Z","shell.execute_reply":"2023-11-28T02:14:42.386392Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 切换运行路径\n%cd /kaggle/working/VADepthNet","metadata":{"execution":{"iopub.status.busy":"2023-11-28T02:14:37.174678Z","iopub.execute_input":"2023-11-28T02:14:37.175021Z","iopub.status.idle":"2023-11-28T02:14:37.180958Z","shell.execute_reply.started":"2023-11-28T02:14:37.174995Z","shell.execute_reply":"2023-11-28T02:14:37.179947Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 切换运行路径\n%cd /kaggle/working/VADepthNet/data_splits","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n重启内存","metadata":{}},{"cell_type":"code","source":"# 重启内存 ！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！重启之后要切换文件夹！！！！！！！！！！！！！！！！！！！！！！！\n# 提示Out Memory可以使用这个代码来格式化运行内存空间\nimport os\nos._exit(00)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **训练+测试+推理--------------------------------------------------------------------------------------**","metadata":{}},{"cell_type":"markdown","source":"---\ntrain训练","metadata":{}},{"cell_type":"code","source":"# train.py--用于训练影像得到模型\n# --------------------------------------------\n# 2023年11月25日22:03:50 尝试使用遥感影像测试，调试了许多的问题，见Endnote\n# 2023年11月26日21:06:09 尝试跑一下rs，然后batchsize设置为3\n# 2023年11月26日22:49:12 出问题了，再次尝试下\n# 2023年11月27日09:36:52 在online—eval的时候报错，修改train.py里面的对应函数重新训练\n# --------------------------------------------\n!python vadepthnet/train.py configs/arguments_train_nyu.txt","metadata":{"execution":{"iopub.status.busy":"2023-11-27T04:43:15.034563Z","iopub.execute_input":"2023-11-27T04:43:15.034932Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\neval评估","metadata":{}},{"cell_type":"code","source":"# eval.py--用于得到精度评定指标\n# --------------------------------------------\n# 2023年11月20日22:26:21 尝试跑通基本的测试，主要修改修改了一下arguments_eval_nyu.txt内容，主要修改了其中的三个路径（对应数据集的位置）和Checkpoint权重文件，以及报错内容\n# --------------------------------------------\n!python vadepthnet/eval.py configs/arguments_eval_nyu.txt","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\ntest测试","metadata":{}},{"cell_type":"code","source":"# test.py--用于测试影像得到深度图\n# --------------------------------------------\n# 2023年11月23日21:18:21 前一天已经能够保存深度图，但是要进一步改进保存深度图和彩色图\n# 2023年11月25日18:12:14 测试两张RS影像（调整大小为NYU相同尺寸，GT也做了调整）\n# 2023年11月28日08:55:45 测试范总主机上训练好的RS权重文件\n# --------------------------------------------\n%run test.py","metadata":{"execution":{"iopub.status.busy":"2023-11-28T02:33:56.140758Z","iopub.execute_input":"2023-11-28T02:33:56.141123Z","iopub.status.idle":"2023-11-28T02:37:00.823570Z","shell.execute_reply.started":"2023-11-28T02:33:56.141094Z","shell.execute_reply":"2023-11-28T02:37:00.822458Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\ndemo案例","metadata":{}},{"cell_type":"code","source":"# 基于author给的案例以及Chat的结果尝试显示深度结果\nimport torch\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nfrom vadepthnet.networks.vadepthnet import VADepthNet\nfrom vadepthnet.dataloaders.dataloader import ToTensor\n\n# 设置设备\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"device: %s\" % device)\n\n# 加载模型\nmodel = VADepthNet(max_depth=10, prior_mean=1.54, img_size=(480, 640))\nmodel = torch.nn.DataParallel(model).to(device)\ncheckpoint = torch.load('/kaggle/input/depthandmodels/vadepthnet_nyu.pth', map_location=device)\nmodel.load_state_dict(checkpoint['model'])\nmodel.eval()\n\n# 图像路径\nimage_path = '/kaggle/working/315000_233500_RGB_0_4_edited.tif'\n\n# 打开图像并进行预处理\nimg = Image.open(image_path)\nimg = np.asarray(img, dtype=np.float32) / 255.0\n\ntotensor = ToTensor('test')\nimg = totensor.to_tensor(img)\nimg = totensor.normalize(img)\nimg = img.unsqueeze(0).to(device)\n\n# 模型推断\npdepth = model.forward(img)\n\n# 将深度值映射到颜色\ndepth_min = 0\ndepth_max = 10\ndepth_colormap = cm.plasma((pdepth.squeeze().detach().cpu().numpy() - depth_min) / (depth_max - depth_min))\n\n# 创建深度图像\ndepth_image = Image.fromarray((depth_colormap * 255).astype(np.uint8))\n\n# 显示深度图像\nplt.imshow(depth_image)\nplt.colorbar(label='Depth')\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\nGitHub回复中四维张量","metadata":{}},{"cell_type":"code","source":"# GItHub中作者的回复，测试单张影像，但是返回的还是张量\nimport torch\nfrom PIL import Image\nimport numpy as np\nfrom vadepthnet.networks.vadepthnet import VADepthNet\nfrom vadepthnet.dataloaders.dataloader import ToTensor\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"device: %s\" % device)\n\nmodel = VADepthNet(max_depth=10,\nprior_mean=1.54,\n# img_size=(500, 500))\nimg_size=(480, 640))\nmodel = torch.nn.DataParallel(model).cuda()\ncheckpoint = torch.load('/kaggle/input/depthandmodels/vadepthnet_nyu.pth', map_location=device)\nmodel.load_state_dict(checkpoint['model'])\nmodel.eval()\n# img = Image.open('/kaggle/input/depthestimation/00000_colors.png')\nimg = Image.open('/kaggle/working/315000_233500_RGB_0_4_edited.tif')\n\nimg = np.asarray(img, dtype=np.float32) / 255.0\n#img = torch.from_numpy(img).cuda().unsqueeze(0)\n\ntotensor = ToTensor('test')\nimg = totensor.to_tensor(img)\nimg = totensor.normalize(img)\nimg = img.unsqueeze(0).cuda()\npdepth = model.forward(img)\nprint(pdepth)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# author在GitHub的问题中的回答，但是返回的是一个四维张量\nimport sys\nsys.path.append('/kaggle/working/VADepthNet')\nimport torch\nfrom PIL import Image\nimport numpy as np\nfrom vadepthnet.networks.vadepthnet import VADepthNet\nfrom vadepthnet.dataloaders.dataloader import ToTensor\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"device: %s\" % device)\n\nmodel = VADepthNet(max_depth=10,\nprior_mean=1.54,\nimg_size=(480, 640))\nmodel = torch.nn.DataParallel(model)\ncheckpoint = torch.load('/kaggle/input/depthandmodels/vadepthnet_nyu.pth', map_location=device)\nmodel.load_state_dict(checkpoint['model'])\nmodel.eval()\nimg = Image.open('/kaggle/input/depthestimation/00001_colors.png')\nimg = np.asarray(img, dtype=np.float32) / 255.0\n#img = torch.from_numpy(img).cuda().unsqueeze(0)\n\ntotensor = ToTensor('test')\nimg = totensor.to_tensor(img)\nimg = totensor.normalize(img)\nimg = img.unsqueeze(0)\npdepth = model.forward(img)\nprint(pdepth)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n数据转换","metadata":{}},{"cell_type":"code","source":"# 转换GT尺寸\nfrom PIL import Image\n\ndef resize_image(input_path, output_path, target_size=(640, 480)):\n    # 打开图像\n    original_image = Image.open(input_path)\n\n    # 获取原始图像的尺寸\n    original_size = original_image.size\n\n    # 计算裁剪的区域\n    left = (original_size[0] - target_size[0]) // 2\n    top = (original_size[1] - target_size[1]) // 2\n    right = (original_size[0] + target_size[0]) // 2\n    bottom = (original_size[1] + target_size[1]) // 2\n\n    # 裁剪图像\n    cropped_image = original_image.crop((left, top, right, bottom))\n\n    # 调整大小\n    resized_image = cropped_image.resize(target_size, Image.ANTIALIAS)\n\n    # 保存结果\n    resized_image.save(output_path, format='PNG')\n\n# 调用函数\ninput_image_path = '/kaggle/input/osidataset/osiDataset/osiDataset/test_heights_adjust/315000_233500_height_0_4.png'\noutput_image_path = '/kaggle/working/315000_233500_height_0_4_edited.png'\nresize_image(input_image_path, output_image_path)\nprint('转换成功！')","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 转换影像大小\nfrom PIL import Image\n\ndef resize_image(input_path, output_path, target_size=(640, 480)):\n    # 打开图像\n    original_image = Image.open(input_path)\n\n    # 获取原始图像的尺寸\n    original_size = original_image.size\n\n    # 计算裁剪的区域\n    left = (original_size[0] - target_size[0]) // 2\n    top = (original_size[1] - target_size[1]) // 2\n    right = (original_size[0] + target_size[0]) // 2\n    bottom = (original_size[1] + target_size[1]) // 2\n\n    # 裁剪图像\n    cropped_image = original_image.crop((left, top, right, bottom))\n\n    # 调整大小\n    resized_image = cropped_image.resize(target_size, Image.ANTIALIAS)\n\n    # 保存结果\n    resized_image.save(output_path)\n\n# 调用函数\ninput_image_path = '/kaggle/input/osidataset/osiDataset/osiDataset/test_rgbs_base/315000_233500_RGB_0_9.tif'\noutput_image_path = '/kaggle/working/315000_233500_RGB_0_9_edited.tif'\nresize_image(input_image_path, output_image_path)\nprint('转换完成！')","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n计算--prior_mean","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\nimport numpy as np\nfrom easydict import EasyDict\nfrom tqdm import tqdm\nfrom fire import Fire\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom _path_init import *\nfrom visualDet3D.networks.utils.registry import DATASET_DICT\nimport visualDet3D.data.kitti.dataset\nfrom visualDet3D.utils.utils import cfg_from_file\n\ndef compute_prior_map(w, h, K):\n\n    x_range = np.arange(w, dtype=np.float32)\n    y_range = np.arange(h, dtype=np.float32)\n    _, yy_grid  = np.meshgrid(x_range, y_range) #[H, W]\n\n    fy =  K[1:2, 1:2] #[1, 1]\n    cy =  K[1:2, 2:3] #[1, 1]\n    Ty =  0\n    \n    relative_elevation = 1.65\n    depth = (fy * relative_elevation + Ty) / (yy_grid - cy + 1e-9) \n    \n    prior = np.zeros_like(depth)\n    mask = yy_grid > cy\n    prior[mask] = np.log(depth[mask])\n    \n    prior[np.logical_not(mask)] = np.log(75)\n\n    prior = np.clip(prior, 0, np.log(75))\n\n    num = np.zeros_like(depth, dtype=np.long)\n    num[mask] = 1000\n    num[np.logical_not(mask)] = 10\n    return prior * num, num","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 计算OSIDataset总的Prior mean\nimport os\nimport numpy as np\nfrom PIL import Image\n\n# 文件夹路径\nfolder_path = \"/kaggle/input/image-depth-estimation/data/nyu2_test\"\n\n# 获取文件夹中所有PNG文件\nimage_files = [f for f in os.listdir(folder_path) if f.endswith('depth.png')]\n\n# 存储每个文件的对数平均值\nlog_means = []\n\n# 遍历每个PNG文件\nfor image_file in image_files:\n    # 读取PNG影像数据\n    img = Image.open(os.path.join(folder_path, image_file))\n    depth_values = np.array(img)\n\n    # 将非零值取对数\n    log_values = np.log(depth_values[depth_values > 0])\n\n    # 计算对数平均值\n    log_mean = np.mean(log_values)\n    \n    # 打印每个文件的对数平均值\n    print(f\"{image_file}: 对数平均值 = {log_mean}\")\n\n    # 将对数平均值添加到列表中\n    log_means.append(log_mean)\n\n# 计算所有文件对数平均值的平均值\noverall_mean = np.mean(log_means)\n\nprint(f\"\\n所有文件对数平均值的平均值 = {overall_mean}\")\n","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 计算OSIDataset总的Prior mean\nimport os\nimport numpy as np\nfrom PIL import Image\n\n# 文件夹路径\nfolder_path = \"/kaggle/input/osidataset/osiDataset/osiDataset/test_heights_adjust\"\n\n# 获取文件夹中所有PNG文件\nimage_files = [f for f in os.listdir(folder_path) if f.endswith('.png')]\n\n# 存储每个文件的对数平均值\nlog_means = []\n\n# 遍历每个PNG文件\nfor image_file in image_files:\n    # 读取PNG影像数据\n    img = Image.open(os.path.join(folder_path, image_file))\n    depth_values = np.array(img)\n\n    # 将非零值取对数\n    log_values = np.log10(depth_values[depth_values > 0])\n\n    # 计算对数平均值\n    log_mean = np.mean(log_values)\n    \n    # 打印每个文件的对数平均值\n    print(f\"{image_file}: 对数平均值 = {log_mean}\")\n\n    # 将对数平均值添加到列表中\n    log_means.append(log_mean)\n\n# 计算所有文件对数平均值的平均值\noverall_mean = np.mean(log_means)\n\nprint(f\"\\n所有文件对数平均值的平均值 = {overall_mean}\")\n","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\n\ndef calculate_prior_mean(dataset_path, filenames_file):\n    # Load filenames from the text file\n    with open(filenames_file, 'r') as file:\n        filenames = file.read().splitlines()\n\n    total_log_depths = 0\n    total_valid_pixels = 0\n\n    for filename in filenames:\n        # Load the depth image (adjust the path accordingly based on your dataset structure)\n        depth_image_path = os.path.join(dataset_path, filename)  # Assuming filenames contain paths\n        depth_image = cv2.imread(depth_image_path, cv2.IMREAD_UNCHANGED).astype(np.float32) / 1000.0  # Assuming depth values are in millimeters\n\n        # Filter out invalid depth values (you may need to adjust this based on your dataset)\n        valid_pixels = depth_image > 0\n\n        # Take the log of valid depth values\n        log_depths = np.log(depth_image[valid_pixels])\n\n        # Update totals\n        total_log_depths += np.sum(log_depths)\n        total_valid_pixels += np.sum(valid_pixels)\n\n    # Calculate the prior mean\n    prior_mean = total_log_depths / total_valid_pixels\n\n    return prior_mean\n\n# Example usage:\ndataset_path = '/kaggle/input/image-depth-estimation/data/nyu2_test'\nfilenames_file = '/kaggle/input/depthandmodels/priormeannyutest.txt'\nprior_mean = calculate_prior_mean(dataset_path, filenames_file)\nprint(f'The calculated prior mean is: {prior_mean}')","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n计算Maxdepth","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom PIL import Image\n\ndef extract_max_depth_and_mean(folder_path):\n    # 获取文件夹中所有PNG文件\n    image_files = [f for f in os.listdir(folder_path) if f.endswith('depth.png')]\n\n    total_max_depth = float('-inf')  # Initialize total_max_depth to negative infinity\n    total_depth_sum = 0\n    total_valid_images = 0\n\n    # 遍历每个PNG文件\n    for image_file in image_files:\n        # 读取PNG影像数据\n        img = Image.open(os.path.join(folder_path, image_file))\n        depth_values = np.array(img)\n\n        # 将非零值取对数\n        valid_pixels = depth_values > 0\n        if np.any(valid_pixels):\n            # Extract max depth for the current image\n            max_depth = np.max(depth_values[valid_pixels])\n\n            # Update total_max_depth\n            total_max_depth = max(total_max_depth, max_depth)\n\n            # Update total_depth_sum\n            total_depth_sum += max_depth\n            total_valid_images += 1\n\n            # Output the max depth for the current image\n            print(f'Max depth for {image_file}: {max_depth}')\n\n    # Calculate the average depth\n    average_depth = total_depth_sum / total_valid_images\n\n    return total_max_depth, average_depth\n\n# Example usage:\nfolder_path = \"/kaggle/input/image-depth-estimation/data/nyu2_test\"\ntotal_max_depth, average_depth = extract_max_depth_and_mean(folder_path)\nprint(f'Total max depth across all images: {total_max_depth}')\nprint(f'Average depth across all images: {average_depth}')","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom PIL import Image\n\ndef extract_max_depth_and_mean(folder_path):\n    # 获取文件夹中所有PNG文件\n    image_files = [f for f in os.listdir(folder_path) if f.endswith('.png')]\n\n    total_max_depth = float('-inf')  # Initialize total_max_depth to negative infinity\n    total_depth_sum = 0\n    total_valid_images = 0\n\n    # 遍历每个PNG文件\n    for image_file in image_files:\n        # 读取PNG影像数据\n        img = Image.open(os.path.join(folder_path, image_file))\n        depth_values = np.array(img)\n\n        # 将非零值取对数\n        valid_pixels = depth_values > 0\n        if np.any(valid_pixels):\n            # Extract max depth for the current image\n            max_depth = np.max(depth_values[valid_pixels])\n\n            # Update total_max_depth\n            total_max_depth = max(total_max_depth, max_depth)\n\n            # Update total_depth_sum\n            total_depth_sum += max_depth\n            total_valid_images += 1\n\n            # Output the max depth for the current image\n            print(f'Max depth for {image_file}: {max_depth}')\n\n    # Calculate the average depth\n    average_depth = total_depth_sum / total_valid_images\n\n    return total_max_depth, average_depth\n\n# Example usage:\n# folder_path = \"/kaggle/input/osidataset/osiDataset/osiDataset/train_heights_adjust\"\nfolder_path = \"/kaggle/input/osidataset/osiDataset/osiDataset/train_heights_adjust\"\ntotal_max_depth, average_depth = extract_max_depth_and_mean(folder_path)\nprint(f'Total max depth across all images: {total_max_depth}')\nprint(f'Average depth across all images: {average_depth}')","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **替换各种内容--------------------------------------------------------------------------------------**","metadata":{}},{"cell_type":"markdown","source":"## **替换train test eval文件**","metadata":{}},{"cell_type":"markdown","source":"/kaggle/working/VADepthNet/vadepthnet/train.py","metadata":{}},{"cell_type":"code","source":"%%writefile /kaggle/working/VADepthNet/vadepthnet/train.py\n# 训练RS影像\nimport torch\nimport torch.nn as nn\nimport torch.nn.utils as utils\nimport torch.backends.cudnn as cudnn\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\n\nimport os, sys, time\nfrom telnetlib import IP\nimport argparse\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom tensorboardX import SummaryWriter\n\nfrom utils import compute_errors, eval_metrics, \\\n                       block_print, enable_print, normalize_result, inv_normalize, convert_arg_line_to_args\nfrom networks.vadepthnet import VADepthNet\n\n\nparser = argparse.ArgumentParser(description='VADepthNet PyTorch implementation.', fromfile_prefix_chars='@')\nparser.convert_arg_line_to_args = convert_arg_line_to_args\n\nparser.add_argument('--mode',                      type=str,   help='train or test', default='train')\nparser.add_argument('--model_name',                type=str,   help='model name', default='vadepthnet')\nparser.add_argument('--pretrain',                  type=str,   help='path of pretrained encoder', default=None)\n\n# Dataset\nparser.add_argument('--dataset',                   type=str,   help='dataset to train on, kitti or nyu', default='nyu')\nparser.add_argument('--data_path',                 type=str,   help='path to the data', required=True)\nparser.add_argument('--gt_path',                   type=str,   help='path to the groundtruth data', required=True)\nparser.add_argument('--filenames_file',            type=str,   help='path to the filenames text file', required=True)\nparser.add_argument('--input_height',              type=int,   help='input height', default=480)\nparser.add_argument('--input_width',               type=int,   help='input width',  default=640)\nparser.add_argument('--max_depth',                 type=float, help='maximum depth in estimation', default=65)\nparser.add_argument('--prior_mean',                type=float, help='prior mean of depth', default=2.58)\n\n# Log and save\nparser.add_argument('--log_directory',             type=str,   help='directory to save checkpoints and summaries', default='')\nparser.add_argument('--checkpoint_path',           type=str,   help='path to a checkpoint to load', default='')\nparser.add_argument('--log_freq',                  type=int,   help='Logging frequency in global steps', default=100)\nparser.add_argument('--save_freq',                 type=int,   help='Checkpoint saving frequency in global steps', default=5000)\n\n# Training\nparser.add_argument('--weight_decay',              type=float, help='weight decay factor for optimization', default=1e-2)\nparser.add_argument('--retrain',                               help='if used with checkpoint_path, will restart training from step zero', action='store_true')\nparser.add_argument('--adam_eps',                  type=float, help='epsilon in Adam optimizer', default=1e-6)\nparser.add_argument('--batch_size',                type=int,   help='batch size', default=4)\nparser.add_argument('--num_epochs',                type=int,   help='number of epochs', default=20)\nparser.add_argument('--learning_rate',             type=float, help='initial learning rate', default=1e-4)\nparser.add_argument('--end_learning_rate',         type=float, help='end learning rate', default=-1)\nparser.add_argument('--variance_focus',            type=float, help='lambda in paper: [0, 1], higher value more focus on minimizing variance of error', default=0.85)\n\n# Preprocessing\nparser.add_argument('--do_random_rotate',                      help='if set, will perform random rotation for augmentation', action='store_true')\nparser.add_argument('--degree',                    type=float, help='random rotation maximum degree', default=2.5)\nparser.add_argument('--do_kb_crop',                            help='if set, crop input images as kitti benchmark images', action='store_true', default= True)\nparser.add_argument('--use_right',                             help='if set, will randomly use right images when train on KITTI', action='store_true')\n\n# Multi-gpu training\nparser.add_argument('--num_threads',               type=int,   help='number of threads to use for data loading', default=1)\nparser.add_argument('--world_size',                type=int,   help='number of nodes for distributed training', default=1)\nparser.add_argument('--rank',                      type=int,   help='node rank for distributed training', default=0)\nparser.add_argument('--dist_url',                  type=str,   help='url used to set up distributed training', default='tcp://127.0.0.1:1234')\nparser.add_argument('--dist_backend',              type=str,   help='distributed backend', default='nccl')\nparser.add_argument('--gpu',                       type=int,   help='GPU id to use.', default=None)\nparser.add_argument('--multiprocessing_distributed',           help='Use multi-processing distributed training to launch '\n                                                                    'N processes per node, which has N GPUs. This is the '\n                                                                    'fastest way to use PyTorch for either single node or '\n                                                                    'multi node data parallel training', action='store_true',)\n# Online eval\nparser.add_argument('--do_online_eval',                        help='if set, perform online eval in every eval_freq steps', action='store_true')\nparser.add_argument('--data_path_eval',            type=str,   help='path to the data for online evaluation', required=False)\nparser.add_argument('--gt_path_eval',              type=str,   help='path to the groundtruth data for online evaluation', required=False)\nparser.add_argument('--filenames_file_eval',       type=str,   help='path to the filenames text file for online evaluation', required=False)\nparser.add_argument('--min_depth_eval',            type=float, help='minimum depth for evaluation', default=0)\nparser.add_argument('--max_depth_eval',            type=float, help='maximum depth for evaluation', default=65)\nparser.add_argument('--eigen_crop',                            help='if set, crops according to Eigen NIPS14', action='store_true')\nparser.add_argument('--garg_crop',                             help='if set, crops according to Garg  ECCV16', action='store_true')\nparser.add_argument('--eval_freq',                 type=int,   help='Online evaluation frequency in global steps', default=500)\nparser.add_argument('--eval_summary_directory',    type=str,   help='output directory for eval summary,'\n                                                                    'if empty outputs to checkpoint folder', default='')\n\nif sys.argv.__len__() == 2:\n    arg_filename_with_prefix = '@' + sys.argv[1]\n    args = parser.parse_args([arg_filename_with_prefix])\nelse:\n    args = parser.parse_args()\n\nif args.dataset == 'kitti' or args.dataset == 'nyu':\n    from dataloaders.dataloader import NewDataLoader\nelif args.dataset == 'kittipred':\n    from dataloaders.dataloader_kittipred import NewDataLoader\n\n\ndef online_eval(model, dataloader_eval, gpu, ngpus, post_process=False):\n    eval_measures = torch.zeros(10).cuda(device=gpu)\n    for _, eval_sample_batched in enumerate(tqdm(dataloader_eval.data)):\n        with torch.no_grad():\n            image = torch.autograd.Variable(eval_sample_batched['image'].cuda(gpu, non_blocking=True))\n            gt_depth = eval_sample_batched['depth']\n            has_valid_depth = eval_sample_batched['has_valid_depth']\n            if not has_valid_depth:\n                # print('Invalid depth. continue.')\n                continue\n\n            pred_depth = model(image)\n\n            pred_depth = pred_depth.cpu().numpy().squeeze()\n            gt_depth = gt_depth.cpu().numpy().squeeze()\n\n        if args.do_kb_crop:\n            height, width = gt_depth.shape\n            top_margin = int(height - 480)\n            left_margin = int((width - 480) / 2)\n            pred_depth_uncropped = np.zeros((height, width), dtype=np.float32)\n            pred_depth_uncropped[top_margin:top_margin + 480, left_margin:left_margin + 480] = pred_depth\n            pred_depth = pred_depth_uncropped\n\n        pred_depth[pred_depth < args.min_depth_eval] = args.min_depth_eval\n        pred_depth[pred_depth > args.max_depth_eval] = args.max_depth_eval\n        pred_depth[np.isinf(pred_depth)] = args.max_depth_eval\n        pred_depth[np.isnan(pred_depth)] = args.min_depth_eval\n\n        valid_mask = np.logical_and(gt_depth > args.min_depth_eval, gt_depth < args.max_depth_eval)\n\n        if args.garg_crop or args.eigen_crop:\n            gt_height, gt_width = gt_depth.shape\n            eval_mask = np.zeros(valid_mask.shape)\n\n            if args.garg_crop:\n                eval_mask[int(0.40810811 * gt_height):int(0.99189189 * gt_height), int(0.03594771 * gt_width):int(0.96405229 * gt_width)] = 1\n#             直接在txt中去掉这个eigen_crop\n            elif args.eigen_crop:\n                if args.dataset == 'kitti':\n                    eval_mask[int(0.3324324 * gt_height):int(0.91351351 * gt_height), int(0.0359477 * gt_width):int(0.96405229 * gt_width)] = 1\n                elif args.dataset == 'nyu':\n                    eval_mask[45:471, 41:601] = 1\n\n            valid_mask = np.logical_and(valid_mask, eval_mask)\n\n        measures = compute_errors(gt_depth[valid_mask], pred_depth[valid_mask])\n\n        eval_measures[:9] += torch.tensor(measures).cuda(device=gpu)\n        eval_measures[9] += 1\n\n    if args.multiprocessing_distributed:\n        group = dist.new_group([i for i in range(ngpus)])\n        dist.all_reduce(tensor=eval_measures, op=dist.ReduceOp.SUM, group=group)\n\n    if not args.multiprocessing_distributed or gpu == 0:\n        eval_measures_cpu = eval_measures.cpu()\n        cnt = eval_measures_cpu[9].item()\n        eval_measures_cpu /= cnt\n        print('Computing errors for {} eval samples'.format(int(cnt)), ', post_process: ', post_process)\n        print(\"{:>7}, {:>7}, {:>7}, {:>7}, {:>7}, {:>7}, {:>7}, {:>7}, {:>7}\".format('silog', 'abs_rel', 'log10', 'rms',\n                                                                                     'sq_rel', 'log_rms', 'd1', 'd2',\n                                                                                     'd3'))\n        for i in range(8):\n            print('{:7.4f}, '.format(eval_measures_cpu[i]), end='')\n        print('{:7.4f}'.format(eval_measures_cpu[8]))\n        return eval_measures_cpu\n\n    return None\n\n\ndef main_worker(gpu, ngpus_per_node, args):\n    args.gpu = gpu\n\n    if args.gpu is not None:\n        print(\"== Use GPU: {} for training\".format(args.gpu))\n\n    if args.distributed:\n        if args.dist_url == \"env://\" and args.rank == -1:\n            args.rank = int(os.environ[\"RANK\"])\n        if args.multiprocessing_distributed:\n            args.rank = args.rank * ngpus_per_node + gpu\n        dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url, world_size=args.world_size, rank=args.rank)\n\n    model = VADepthNet(pretrained=args.pretrain,\n                       max_depth=args.max_depth,\n                       prior_mean=args.prior_mean,\n                       img_size=(args.input_height, args.input_width))\n    model.train()\n\n    num_params = sum([np.prod(p.size()) for p in model.parameters()])\n    print(\"== Total number of parameters: {}\".format(num_params))\n\n    num_params_update = sum([np.prod(p.shape) for p in model.parameters() if p.requires_grad])\n    print(\"== Total number of learning parameters: {}\".format(num_params_update))\n\n    if args.distributed:\n        if args.gpu is not None:\n            torch.cuda.set_device(args.gpu)\n            model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n            model.cuda(args.gpu)\n            args.batch_size = int(args.batch_size / ngpus_per_node)\n            model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu], find_unused_parameters=True)\n        else:\n            model.cuda()\n            model = torch.nn.parallel.DistributedDataParallel(model, find_unused_parameters=True)\n    else:\n        model = torch.nn.DataParallel(model)\n        model.cuda()\n\n    if args.distributed:\n        print(\"== Model Initialized on GPU: {}\".format(args.gpu))\n    else:\n        print(\"== Model Initialized\")\n\n    global_step = 0\n    best_eval_measures_lower_better = torch.zeros(6).cpu() + 1e3\n    best_eval_measures_higher_better = torch.zeros(3).cpu()\n    best_eval_steps = np.zeros(9, dtype=np.int32)\n\n    # Training parameters\n    optimizer = torch.optim.Adam([{'params': model.module.parameters()}],\n                                lr=args.learning_rate,\n                                weight_decay=args.weight_decay)\n\n    model_just_loaded = False\n    if args.checkpoint_path != '':\n        if os.path.isfile(args.checkpoint_path):\n            print(\"== Loading checkpoint '{}'\".format(args.checkpoint_path))\n            if args.gpu is None:\n                checkpoint = torch.load(args.checkpoint_path)\n            else:\n                loc = 'cuda:{}'.format(args.gpu)\n                checkpoint = torch.load(args.checkpoint_path, map_location=loc)\n            model.load_state_dict(checkpoint['model'])\n            optimizer.load_state_dict(checkpoint['optimizer'])\n            if not args.retrain:\n                try:\n                    global_step = checkpoint['global_step']\n                    best_eval_measures_higher_better = checkpoint['best_eval_measures_higher_better'].cpu()\n                    best_eval_measures_lower_better = checkpoint['best_eval_measures_lower_better'].cpu()\n                    best_eval_steps = checkpoint['best_eval_steps']\n                except KeyError:\n                    print(\"Could not load values for online evaluation\")\n\n            print(\"== Loaded checkpoint '{}' (global_step {})\".format(args.checkpoint_path, checkpoint['global_step']))\n        else:\n            print(\"== No checkpoint found at '{}'\".format(args.checkpoint_path))\n        model_just_loaded = True\n        del checkpoint\n\n    cudnn.benchmark = True\n\n    dataloader = NewDataLoader(args, 'train')\n    dataloader_eval = NewDataLoader(args, 'online_eval')\n\n    # ===== Evaluation before training ======\n    # model.eval()\n    # with torch.no_grad():\n    #     eval_measures = online_eval(model, dataloader_eval, gpu, ngpus_per_node, post_process=True)\n\n    # Logging\n    if not args.multiprocessing_distributed or (args.multiprocessing_distributed and args.rank % ngpus_per_node == 0):\n        writer = SummaryWriter(args.log_directory + '/' + args.model_name + '/summaries', flush_secs=30)\n        if args.do_online_eval:\n            if args.eval_summary_directory != '':\n                eval_summary_path = os.path.join(args.eval_summary_directory, args.model_name)\n            else:\n                eval_summary_path = os.path.join(args.log_directory, args.model_name, 'eval')\n            eval_summary_writer = SummaryWriter(eval_summary_path, flush_secs=30)\n\n    #silog_criterion = silog_loss(variance_focus=args.variance_focus)\n\n    start_time = time.time()\n    duration = 0\n\n    num_log_images = args.batch_size\n    end_learning_rate = args.end_learning_rate if args.end_learning_rate != -1 else 0.1 * args.learning_rate\n\n    var_sum = [var.sum().item() for var in model.parameters() if var.requires_grad]\n    var_cnt = len(var_sum)\n    var_sum = np.sum(var_sum)\n\n    print(\"== Initial variables' sum: {:.3f}, avg: {:.3f}\".format(var_sum, var_sum/var_cnt))\n\n    steps_per_epoch = len(dataloader.data)\n    num_total_steps = args.num_epochs * steps_per_epoch\n    epoch = global_step // steps_per_epoch\n\n    while epoch < args.num_epochs:\n        if args.distributed:\n            dataloader.train_sampler.set_epoch(epoch)\n\n        for step, sample_batched in enumerate(dataloader.data):\n            optimizer.zero_grad()\n            before_op_time = time.time()\n\n            image = torch.autograd.Variable(sample_batched['image'].cuda(args.gpu, non_blocking=True))\n            depth_gt = torch.autograd.Variable(sample_batched['depth'].cuda(args.gpu, non_blocking=True))\n\n            depth_est, loss = model(image, depth_gt)\n\n            loss.backward()\n\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            \n            optimizer.step()\n\n            for param_group in optimizer.param_groups:\n                current_lr = (args.learning_rate - end_learning_rate) * (1 - global_step / num_total_steps) ** 0.9 + end_learning_rate\n                param_group['lr'] = current_lr\n\n            #optimizer.step()\n\n\n            duration += time.time() - before_op_time\n            if global_step and global_step % args.log_freq == 0 and not model_just_loaded and gpu == 0:\n\n                print('epoch:', epoch, 'global_step:', global_step, 'loss:', loss.item(), flush=True)\n\n            if args.do_online_eval and global_step and global_step % args.eval_freq == 0 and not model_just_loaded:\n                time.sleep(0.1)\n                model.eval()\n                with torch.no_grad():\n                    eval_measures = online_eval(model, dataloader_eval, gpu, ngpus_per_node, post_process=False)\n                if eval_measures is not None:\n                    for i in range(9):\n                        eval_summary_writer.add_scalar(eval_metrics[i], eval_measures[i].cpu(), int(global_step))\n                        measure = eval_measures[i]\n                        is_best = False\n                        if i < 6 and measure < best_eval_measures_lower_better[i]:\n                            old_best = best_eval_measures_lower_better[i].item()\n                            best_eval_measures_lower_better[i] = measure.item()\n                            is_best = True\n                        elif i >= 6 and measure > best_eval_measures_higher_better[i-6]:\n                            old_best = best_eval_measures_higher_better[i-6].item()\n                            best_eval_measures_higher_better[i-6] = measure.item()\n                            is_best = True\n                        if is_best:\n                            old_best_step = best_eval_steps[i]\n                            old_best_name = '/model-{}-best_{}_{:.5f}'.format(old_best_step, eval_metrics[i], old_best)\n                            model_path = args.log_directory + '/' + args.model_name + old_best_name\n                            if os.path.exists(model_path):\n                                command = 'rm {}'.format(model_path)\n                                os.system(command)\n                            best_eval_steps[i] = global_step\n                            model_save_name = '/model-{}-best_{}_{:.5f}'.format(global_step, eval_metrics[i], measure)\n                            print('New best for {}. Saving model: {}'.format(eval_metrics[i], model_save_name))\n                            checkpoint = {'global_step': global_step,\n                                          'model': model.state_dict(),\n                                          #'optimizer': optimizer.state_dict(),\n                                          'best_eval_measures_higher_better': best_eval_measures_higher_better,\n                                          'best_eval_measures_lower_better': best_eval_measures_lower_better,\n                                          'best_eval_steps': best_eval_steps\n                                          }\n                            torch.save(checkpoint, args.log_directory + '/' + args.model_name + model_save_name)\n                    eval_summary_writer.flush()\n                model.train()\n                block_print()\n                enable_print()\n            model_just_loaded = False\n            global_step += 1\n\n        epoch += 1\n       \n    if not args.multiprocessing_distributed or (args.multiprocessing_distributed and args.rank % ngpus_per_node == 0):\n        writer.close()\n        if args.do_online_eval:\n            eval_summary_writer.close()\n\n\ndef main():\n    if args.mode != 'train':\n        print('train.py is only for training.')\n        return -1\n\n    command = 'mkdir ' + os.path.join(args.log_directory, args.model_name)\n    os.system(command)\n\n    args_out_path = os.path.join(args.log_directory, args.model_name)\n    command = 'cp ' + sys.argv[1] + ' ' + args_out_path\n    os.system(command)\n\n    torch.cuda.empty_cache()\n    args.distributed = args.world_size > 1 or args.multiprocessing_distributed\n\n    ngpus_per_node = torch.cuda.device_count()\n    if ngpus_per_node > 1 and not args.multiprocessing_distributed:\n        print(\"This machine has more than 1 gpu. Please specify --multiprocessing_distributed, or set \\'CUDA_VISIBLE_DEVICES=0\\'\")\n        return -1\n\n    if args.do_online_eval:\n        print(\"You have specified --do_online_eval.\")\n        print(\"This will evaluate the model every eval_freq {} steps and save best models for individual eval metrics.\"\n              .format(args.eval_freq))\n\n    if args.multiprocessing_distributed:\n        args.world_size = ngpus_per_node * args.world_size\n        mp.spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, args))\n    else:\n        main_worker(args.gpu, ngpus_per_node, args)\n\n\nif __name__ == '__main__':\n    main()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile /kaggle/working/VADepthNet/vadepthnet/train.py\n# 训练RS影像\nimport torch\nimport torch.nn as nn\nimport torch.nn.utils as utils\nimport torch.backends.cudnn as cudnn\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\n\nimport os, sys, time\nfrom telnetlib import IP\nimport argparse\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom tensorboardX import SummaryWriter\n\nfrom utils import compute_errors, eval_metrics, \\\n                       block_print, enable_print, normalize_result, inv_normalize, convert_arg_line_to_args\nfrom networks.vadepthnet import VADepthNet\n\n\nparser = argparse.ArgumentParser(description='VADepthNet PyTorch implementation.', fromfile_prefix_chars='@')\nparser.convert_arg_line_to_args = convert_arg_line_to_args\n\nparser.add_argument('--mode',                      type=str,   help='train or test', default='train')\nparser.add_argument('--model_name',                type=str,   help='model name', default='vadepthnet')\nparser.add_argument('--pretrain',                  type=str,   help='path of pretrained encoder', default=None)\n\n# Dataset\nparser.add_argument('--dataset',                   type=str,   help='dataset to train on, kitti or nyu', default='nyu')\nparser.add_argument('--data_path',                 type=str,   help='path to the data', required=True)\nparser.add_argument('--gt_path',                   type=str,   help='path to the groundtruth data', required=True)\nparser.add_argument('--filenames_file',            type=str,   help='path to the filenames text file', required=True)\nparser.add_argument('--input_height',              type=int,   help='input height', default=480)\nparser.add_argument('--input_width',               type=int,   help='input width',  default=640)\nparser.add_argument('--max_depth',                 type=float, help='maximum depth in estimation', default=10)\nparser.add_argument('--prior_mean',                type=float, help='prior mean of depth', default=2.58)\n\n# Log and save\nparser.add_argument('--log_directory',             type=str,   help='directory to save checkpoints and summaries', default='')\nparser.add_argument('--checkpoint_path',           type=str,   help='path to a checkpoint to load', default='')\nparser.add_argument('--log_freq',                  type=int,   help='Logging frequency in global steps', default=100)\nparser.add_argument('--save_freq',                 type=int,   help='Checkpoint saving frequency in global steps', default=5000)\n\n# Training\nparser.add_argument('--weight_decay',              type=float, help='weight decay factor for optimization', default=1e-2)\nparser.add_argument('--retrain',                               help='if used with checkpoint_path, will restart training from step zero', action='store_true')\nparser.add_argument('--adam_eps',                  type=float, help='epsilon in Adam optimizer', default=1e-6)\nparser.add_argument('--batch_size',                type=int,   help='batch size', default=4)\nparser.add_argument('--num_epochs',                type=int,   help='number of epochs', default=50)\nparser.add_argument('--learning_rate',             type=float, help='initial learning rate', default=1e-4)\nparser.add_argument('--end_learning_rate',         type=float, help='end learning rate', default=-1)\nparser.add_argument('--variance_focus',            type=float, help='lambda in paper: [0, 1], higher value more focus on minimizing variance of error', default=0.85)\n\n# Preprocessing\nparser.add_argument('--do_random_rotate',                      help='if set, will perform random rotation for augmentation', action='store_true')\nparser.add_argument('--degree',                    type=float, help='random rotation maximum degree', default=2.5)\nparser.add_argument('--do_kb_crop',                            help='if set, crop input images as kitti benchmark images', action='store_true')\nparser.add_argument('--use_right',                             help='if set, will randomly use right images when train on KITTI', action='store_true')\n\n# Multi-gpu training\nparser.add_argument('--num_threads',               type=int,   help='number of threads to use for data loading', default=1)\nparser.add_argument('--world_size',                type=int,   help='number of nodes for distributed training', default=1)\nparser.add_argument('--rank',                      type=int,   help='node rank for distributed training', default=0)\nparser.add_argument('--dist_url',                  type=str,   help='url used to set up distributed training', default='tcp://127.0.0.1:1234')\nparser.add_argument('--dist_backend',              type=str,   help='distributed backend', default='nccl')\nparser.add_argument('--gpu',                       type=int,   help='GPU id to use.', default=None)\nparser.add_argument('--multiprocessing_distributed',           help='Use multi-processing distributed training to launch '\n                                                                    'N processes per node, which has N GPUs. This is the '\n                                                                    'fastest way to use PyTorch for either single node or '\n                                                                    'multi node data parallel training', action='store_true',)\n# Online eval\nparser.add_argument('--do_online_eval',                        help='if set, perform online eval in every eval_freq steps', action='store_true')\nparser.add_argument('--data_path_eval',            type=str,   help='path to the data for online evaluation', required=False)\nparser.add_argument('--gt_path_eval',              type=str,   help='path to the groundtruth data for online evaluation', required=False)\nparser.add_argument('--filenames_file_eval',       type=str,   help='path to the filenames text file for online evaluation', required=False)\nparser.add_argument('--min_depth_eval',            type=float, help='minimum depth for evaluation', default=1e-3)\nparser.add_argument('--max_depth_eval',            type=float, help='maximum depth for evaluation', default=80)\nparser.add_argument('--eigen_crop',                            help='if set, crops according to Eigen NIPS14', action='store_true')\nparser.add_argument('--garg_crop',                             help='if set, crops according to Garg  ECCV16', action='store_true')\nparser.add_argument('--eval_freq',                 type=int,   help='Online evaluation frequency in global steps', default=500)\nparser.add_argument('--eval_summary_directory',    type=str,   help='output directory for eval summary,'\n                                                                    'if empty outputs to checkpoint folder', default='')\n\nif sys.argv.__len__() == 2:\n    arg_filename_with_prefix = '@' + sys.argv[1]\n    args = parser.parse_args([arg_filename_with_prefix])\nelse:\n    args = parser.parse_args()\n\nif args.dataset == 'kitti' or args.dataset == 'nyu':\n    from dataloaders.dataloader import NewDataLoader\nelif args.dataset == 'kittipred':\n    from dataloaders.dataloader_kittipred import NewDataLoader\n\n\ndef online_eval(model, dataloader_eval, gpu, ngpus, post_process=False):\n    eval_measures = torch.zeros(10).cuda(device=gpu)\n    for _, eval_sample_batched in enumerate(tqdm(dataloader_eval.data)):\n        with torch.no_grad():\n            image = torch.autograd.Variable(eval_sample_batched['image'].cuda(gpu, non_blocking=True))\n            gt_depth = eval_sample_batched['depth']\n            has_valid_depth = eval_sample_batched['has_valid_depth']\n            if not has_valid_depth:\n                # print('Invalid depth. continue.')\n                continue\n\n            pred_depth = model(image)\n\n            pred_depth = pred_depth.cpu().numpy().squeeze()\n            gt_depth = gt_depth.cpu().numpy().squeeze()\n\n        if args.do_kb_crop:\n            height, width = gt_depth.shape\n            top_margin = int(height - 352)\n            left_margin = int((width - 1216) / 2)\n            pred_depth_uncropped = np.zeros((height, width), dtype=np.float32)\n            pred_depth_uncropped[top_margin:top_margin + 352, left_margin:left_margin + 1216] = pred_depth\n            pred_depth = pred_depth_uncropped\n\n        pred_depth[pred_depth < args.min_depth_eval] = args.min_depth_eval\n        pred_depth[pred_depth > args.max_depth_eval] = args.max_depth_eval\n        pred_depth[np.isinf(pred_depth)] = args.max_depth_eval\n        pred_depth[np.isnan(pred_depth)] = args.min_depth_eval\n\n        valid_mask = np.logical_and(gt_depth > args.min_depth_eval, gt_depth < args.max_depth_eval)\n\n        if args.garg_crop or args.eigen_crop:\n            gt_height, gt_width = gt_depth.shape\n            eval_mask = np.zeros(valid_mask.shape)\n\n            if args.garg_crop:\n                eval_mask[int(0.40810811 * gt_height):int(0.99189189 * gt_height), int(0.03594771 * gt_width):int(0.96405229 * gt_width)] = 1\n\n            elif args.eigen_crop:\n                if args.dataset == 'kitti':\n                    eval_mask[int(0.3324324 * gt_height):int(0.91351351 * gt_height), int(0.0359477 * gt_width):int(0.96405229 * gt_width)] = 1\n                elif args.dataset == 'nyu':\n                    eval_mask[45:471, 41:601] = 1\n\n            valid_mask = np.logical_and(valid_mask, eval_mask)\n\n        measures = compute_errors(gt_depth[valid_mask], pred_depth[valid_mask])\n\n        eval_measures[:9] += torch.tensor(measures).cuda(device=gpu)\n        eval_measures[9] += 1\n\n    if args.multiprocessing_distributed:\n        group = dist.new_group([i for i in range(ngpus)])\n        dist.all_reduce(tensor=eval_measures, op=dist.ReduceOp.SUM, group=group)\n\n    if not args.multiprocessing_distributed or gpu == 0:\n        eval_measures_cpu = eval_measures.cpu()\n        cnt = eval_measures_cpu[9].item()\n        eval_measures_cpu /= cnt\n        print('Computing errors for {} eval samples'.format(int(cnt)), ', post_process: ', post_process)\n        print(\"{:>7}, {:>7}, {:>7}, {:>7}, {:>7}, {:>7}, {:>7}, {:>7}, {:>7}\".format('silog', 'abs_rel', 'log10', 'rms',\n                                                                                     'sq_rel', 'log_rms', 'd1', 'd2',\n                                                                                     'd3'))\n        for i in range(8):\n            print('{:7.4f}, '.format(eval_measures_cpu[i]), end='')\n        print('{:7.4f}'.format(eval_measures_cpu[8]))\n        return eval_measures_cpu\n\n    return None\n\n\ndef main_worker(gpu, ngpus_per_node, args):\n    args.gpu = gpu\n\n    if args.gpu is not None:\n        print(\"== Use GPU: {} for training\".format(args.gpu))\n\n    if args.distributed:\n        if args.dist_url == \"env://\" and args.rank == -1:\n            args.rank = int(os.environ[\"RANK\"])\n        if args.multiprocessing_distributed:\n            args.rank = args.rank * ngpus_per_node + gpu\n        dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url, world_size=args.world_size, rank=args.rank)\n\n    model = VADepthNet(pretrained=args.pretrain,\n                       max_depth=args.max_depth,\n                       prior_mean=args.prior_mean,\n                       img_size=(args.input_height, args.input_width))\n    model.train()\n\n    num_params = sum([np.prod(p.size()) for p in model.parameters()])\n    print(\"== Total number of parameters: {}\".format(num_params))\n\n    num_params_update = sum([np.prod(p.shape) for p in model.parameters() if p.requires_grad])\n    print(\"== Total number of learning parameters: {}\".format(num_params_update))\n\n    if args.distributed:\n        if args.gpu is not None:\n            torch.cuda.set_device(args.gpu)\n            model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n            model.cuda(args.gpu)\n            args.batch_size = int(args.batch_size / ngpus_per_node)\n            model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu], find_unused_parameters=True)\n        else:\n            model.cuda()\n            model = torch.nn.parallel.DistributedDataParallel(model, find_unused_parameters=True)\n    else:\n        model = torch.nn.DataParallel(model)\n        model.cuda()\n\n    if args.distributed:\n        print(\"== Model Initialized on GPU: {}\".format(args.gpu))\n    else:\n        print(\"== Model Initialized\")\n\n    global_step = 0\n    best_eval_measures_lower_better = torch.zeros(6).cpu() + 1e3\n    best_eval_measures_higher_better = torch.zeros(3).cpu()\n    best_eval_steps = np.zeros(9, dtype=np.int32)\n\n    # Training parameters\n    optimizer = torch.optim.Adam([{'params': model.module.parameters()}],\n                                lr=args.learning_rate,\n                                weight_decay=args.weight_decay)\n\n    model_just_loaded = False\n    if args.checkpoint_path != '':\n        if os.path.isfile(args.checkpoint_path):\n            print(\"== Loading checkpoint '{}'\".format(args.checkpoint_path))\n            if args.gpu is None:\n                checkpoint = torch.load(args.checkpoint_path)\n            else:\n                loc = 'cuda:{}'.format(args.gpu)\n                checkpoint = torch.load(args.checkpoint_path, map_location=loc)\n            model.load_state_dict(checkpoint['model'])\n            optimizer.load_state_dict(checkpoint['optimizer'])\n            if not args.retrain:\n                try:\n                    global_step = checkpoint['global_step']\n                    best_eval_measures_higher_better = checkpoint['best_eval_measures_higher_better'].cpu()\n                    best_eval_measures_lower_better = checkpoint['best_eval_measures_lower_better'].cpu()\n                    best_eval_steps = checkpoint['best_eval_steps']\n                except KeyError:\n                    print(\"Could not load values for online evaluation\")\n\n            print(\"== Loaded checkpoint '{}' (global_step {})\".format(args.checkpoint_path, checkpoint['global_step']))\n        else:\n            print(\"== No checkpoint found at '{}'\".format(args.checkpoint_path))\n        model_just_loaded = True\n        del checkpoint\n\n    cudnn.benchmark = True\n\n    dataloader = NewDataLoader(args, 'train')\n    dataloader_eval = NewDataLoader(args, 'online_eval')\n\n    # ===== Evaluation before training ======\n    # model.eval()\n    # with torch.no_grad():\n    #     eval_measures = online_eval(model, dataloader_eval, gpu, ngpus_per_node, post_process=True)\n\n    # Logging\n    if not args.multiprocessing_distributed or (args.multiprocessing_distributed and args.rank % ngpus_per_node == 0):\n        writer = SummaryWriter(args.log_directory + '/' + args.model_name + '/summaries', flush_secs=30)\n        if args.do_online_eval:\n            if args.eval_summary_directory != '':\n                eval_summary_path = os.path.join(args.eval_summary_directory, args.model_name)\n            else:\n                eval_summary_path = os.path.join(args.log_directory, args.model_name, 'eval')\n            eval_summary_writer = SummaryWriter(eval_summary_path, flush_secs=30)\n\n    #silog_criterion = silog_loss(variance_focus=args.variance_focus)\n\n    start_time = time.time()\n    duration = 0\n\n    num_log_images = args.batch_size\n    end_learning_rate = args.end_learning_rate if args.end_learning_rate != -1 else 0.1 * args.learning_rate\n\n    var_sum = [var.sum().item() for var in model.parameters() if var.requires_grad]\n    var_cnt = len(var_sum)\n    var_sum = np.sum(var_sum)\n\n    print(\"== Initial variables' sum: {:.3f}, avg: {:.3f}\".format(var_sum, var_sum/var_cnt))\n\n    steps_per_epoch = len(dataloader.data)\n    num_total_steps = args.num_epochs * steps_per_epoch\n    epoch = global_step // steps_per_epoch\n\n    while epoch < args.num_epochs:\n        if args.distributed:\n            dataloader.train_sampler.set_epoch(epoch)\n\n        for step, sample_batched in enumerate(dataloader.data):\n            optimizer.zero_grad()\n            before_op_time = time.time()\n\n            image = torch.autograd.Variable(sample_batched['image'].cuda(args.gpu, non_blocking=True))\n            depth_gt = torch.autograd.Variable(sample_batched['depth'].cuda(args.gpu, non_blocking=True))\n\n            depth_est, loss = model(image, depth_gt)\n\n            loss.backward()\n\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            \n            optimizer.step()\n\n            for param_group in optimizer.param_groups:\n                current_lr = (args.learning_rate - end_learning_rate) * (1 - global_step / num_total_steps) ** 0.9 + end_learning_rate\n                param_group['lr'] = current_lr\n\n            #optimizer.step()\n\n\n            duration += time.time() - before_op_time\n            if global_step and global_step % args.log_freq == 0 and not model_just_loaded and gpu == 0:\n\n                print('epoch:', epoch, 'global_step:', global_step, 'loss:', loss.item(), flush=True)\n\n            if args.do_online_eval and global_step and global_step % args.eval_freq == 0 and not model_just_loaded:\n                time.sleep(0.1)\n                model.eval()\n                with torch.no_grad():\n                    eval_measures = online_eval(model, dataloader_eval, gpu, ngpus_per_node, post_process=False)\n                if eval_measures is not None:\n                    for i in range(9):\n                        eval_summary_writer.add_scalar(eval_metrics[i], eval_measures[i].cpu(), int(global_step))\n                        measure = eval_measures[i]\n                        is_best = False\n                        if i < 6 and measure < best_eval_measures_lower_better[i]:\n                            old_best = best_eval_measures_lower_better[i].item()\n                            best_eval_measures_lower_better[i] = measure.item()\n                            is_best = True\n                        elif i >= 6 and measure > best_eval_measures_higher_better[i-6]:\n                            old_best = best_eval_measures_higher_better[i-6].item()\n                            best_eval_measures_higher_better[i-6] = measure.item()\n                            is_best = True\n                        if is_best:\n                            old_best_step = best_eval_steps[i]\n                            old_best_name = '/model-{}-best_{}_{:.5f}'.format(old_best_step, eval_metrics[i], old_best)\n                            model_path = args.log_directory + '/' + args.model_name + old_best_name\n                            if os.path.exists(model_path):\n                                command = 'rm {}'.format(model_path)\n                                os.system(command)\n                            best_eval_steps[i] = global_step\n                            model_save_name = '/model-{}-best_{}_{:.5f}'.format(global_step, eval_metrics[i], measure)\n                            print('New best for {}. Saving model: {}'.format(eval_metrics[i], model_save_name))\n                            checkpoint = {'global_step': global_step,\n                                          'model': model.state_dict(),\n                                          #'optimizer': optimizer.state_dict(),\n                                          'best_eval_measures_higher_better': best_eval_measures_higher_better,\n                                          'best_eval_measures_lower_better': best_eval_measures_lower_better,\n                                          'best_eval_steps': best_eval_steps\n                                          }\n                            torch.save(checkpoint, args.log_directory + '/' + args.model_name + model_save_name)\n                    eval_summary_writer.flush()\n                model.train()\n                block_print()\n                enable_print()\n\n            model_just_loaded = False\n            global_step += 1\n\n        epoch += 1\n       \n    if not args.multiprocessing_distributed or (args.multiprocessing_distributed and args.rank % ngpus_per_node == 0):\n        writer.close()\n        if args.do_online_eval:\n            eval_summary_writer.close()\n\n\ndef main():\n    if args.mode != 'train':\n        print('train.py is only for training.')\n        return -1\n\n    command = 'mkdir ' + os.path.join(args.log_directory, args.model_name)\n    os.system(command)\n\n    args_out_path = os.path.join(args.log_directory, args.model_name)\n    command = 'cp ' + sys.argv[1] + ' ' + args_out_path\n    os.system(command)\n\n    torch.cuda.empty_cache()\n    args.distributed = args.world_size > 1 or args.multiprocessing_distributed\n\n    ngpus_per_node = torch.cuda.device_count()\n    if ngpus_per_node > 1 and not args.multiprocessing_distributed:\n        print(\"This machine has more than 1 gpu. Please specify --multiprocessing_distributed, or set \\'CUDA_VISIBLE_DEVICES=0\\'\")\n        return -1\n\n    if args.do_online_eval:\n        print(\"You have specified --do_online_eval.\")\n        print(\"This will evaluate the model every eval_freq {} steps and save best models for individual eval metrics.\"\n              .format(args.eval_freq))\n\n    if args.multiprocessing_distributed:\n        args.world_size = ngpus_per_node * args.world_size\n        mp.spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, args))\n    else:\n        main_worker(args.gpu, ngpus_per_node, args)\n\n\nif __name__ == '__main__':\n    main()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n/kaggle/working/VADepthNet/vadepthnet/test.py","metadata":{}},{"cell_type":"code","source":"%%writefile /kaggle/working/VADepthNet/vadepthnet/test.py\n# 测试两张RS影像查看结果\nfrom __future__ import absolute_import, division, print_function\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nimport os, sys, errno\nimport argparse\nimport time\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\nfrom utils import post_process_depth, flip_lr\nfrom networks.vadepthnet import VADepthNet\n\nimport logging\n# 创建一个记录器实例\nlogger = logging.getLogger(__name__)\n\ndef convert_arg_line_to_args(arg_line):\n    for arg in arg_line.split():\n        if not arg.strip():\n            continue\n        yield arg\n\n\nparser = argparse.ArgumentParser(description='VADepthNet PyTorch implementation.', fromfile_prefix_chars='@')\nparser.convert_arg_line_to_args = convert_arg_line_to_args\n\n# --pretrain /kaggle/input/depthandmodels/swin_base_patch4_window12_384_22k.pth\nparser.add_argument('--pretrain', type=str,   help='path of pretrained encoder', default=None)\nparser.add_argument('--prior_mean', type=float, help='mean value for prior', default=2.58)\n\nparser.add_argument('--model_name', type=str, help='model name', default='vadepthnet')\n# parser.add_argument('--encoder', type=str, help='type of encoder, base07, large07', default='large07')\nparser.add_argument('--data_path', type=str, help='path to the data', default='/kaggle/input/osidataset/osiDataset/osiDataset')\nparser.add_argument('--filenames_file', type=str, help='path to the filenames text file', default='/kaggle/input/smallfile/ositest1.txt')\nparser.add_argument('--input_height', type=int, help='input height', default=480)\nparser.add_argument('--input_width', type=int, help='input width', default=480)\nparser.add_argument('--max_depth', type=float, help='maximum depth in estimation', default=65.6)\n# parser.add_argument('--checkpoint_path', type=str, help='path to a specific checkpoint to load', default='/kaggle/input/depthandmodels/vadepthnet_nyu.pth')\nparser.add_argument('--checkpoint_path', type=str, help='path to a specific checkpoint to load', default='/kaggle/working/models/vadepthnet/model-13000-best_rms_4.38934')\nparser.add_argument('--dataset', type=str, help='dataset to train on', default='nyu')\n# 这个参数是将数据集裁剪为Kitti以及benchmark数据集的尺寸大小\nparser.add_argument('--do_kb_crop', help='if set, crop input images as kitti benchmark images', action='store_true', default=True)\n# 这个参数是用来设置是否需要保存彩色深度结果、原图、和GT图像，已注释后两者\nparser.add_argument('--save_viz', help='if set, save visulization of the outputs', action='store_true', default=True)\n\nif sys.argv.__len__() == 2:\n    arg_filename_with_prefix = '@' + sys.argv[1]\n    args = parser.parse_args([arg_filename_with_prefix])\nelse:\n    args = parser.parse_args()\n\nif args.dataset == 'kitti' or args.dataset == 'nyu':\n    from dataloaders.dataloader import NewDataLoader\nelif args.dataset == 'kittipred':\n    from dataloaders.dataloader_kittipred import NewDataLoader\n\nmodel_dir = os.path.dirname(args.checkpoint_path)\nsys.path.append(model_dir)\n\n\ndef get_num_lines(file_path):\n    f = open(file_path, 'r')\n    lines = f.readlines()\n    f.close()\n    return len(lines)\n\n\ndef test(params):\n    \"\"\"Test function.\"\"\"\n    args.mode = 'test'\n    dataloader = NewDataLoader(args, 'test')\n    \n#     model = NewCRFDepth(version='large07', inv_depth=False, max_depth=args.max_depth)\n#     model = VADepthNet(version='large07', inv_depth=False, max_depth=args.max_depth)\n    model = VADepthNet(pretrained=args.pretrain, max_depth=args.max_depth, prior_mean=args.prior_mean,\n                       img_size=(args.input_height, args.input_width))\n\n    model = torch.nn.DataParallel(model)\n    \n    checkpoint = torch.load(args.checkpoint_path)\n    model.load_state_dict(checkpoint['model'])\n    model.eval()\n    model.cuda()\n\n    num_params = sum([np.prod(p.size()) for p in model.parameters()])\n    print(\"Total number of parameters: {}\".format(num_params))\n\n    num_test_samples = get_num_lines(args.filenames_file)\n\n    with open(args.filenames_file) as f:\n        lines = f.readlines()\n\n#     print('now testing {} files with {}'.format(num_test_samples, args.checkpoint_path))\n    print('now testing {} files with {}'.format(args.checkpoint_path, num_test_samples))\n\n    pred_depths = []\n    start_time = time.time()\n    with torch.no_grad():\n        for _, sample in enumerate(tqdm(dataloader.data)):\n            image = Variable(sample['image'].cuda())\n            # Predict\n            depth_est = model(image)\n            post_process = True\n            if post_process:\n                image_flipped = flip_lr(image)\n                depth_est_flipped = model(image_flipped)\n                depth_est = post_process_depth(depth_est, depth_est_flipped)\n\n            pred_depth = depth_est.cpu().numpy().squeeze()\n\n            if args.do_kb_crop:\n                height, width = 480, 480\n                top_margin = int((height - 480) / 2)\n                left_margin = int((width - 480) / 2)\n                pred_depth_uncropped = np.zeros((height, width), dtype=np.float32)\n                pred_depth_uncropped[top_margin:top_margin + 480, left_margin:left_margin + 480] = pred_depth\n                pred_depth = pred_depth_uncropped\n\n            pred_depths.append(pred_depth)\n\n    elapsed_time = time.time() - start_time\n    print('Elapesed time: %s' % str(elapsed_time))\n    print('Done.')\n    \n    save_name = '/kaggle/working/result_' + args.model_name\n    \n    print('Saving result pngs..')\n    if not os.path.exists(save_name):\n        try:\n            os.mkdir(save_name)\n            os.mkdir(save_name + '/raw')\n            os.mkdir(save_name + '/cmap')\n            os.mkdir(save_name + '/rgb')\n            os.mkdir(save_name + '/gt')\n        except OSError as e:\n            if e.errno != errno.EEXIST:\n                raise\n    \n    for s in tqdm(range(num_test_samples)):\n        if args.dataset == 'kitti':\n            date_drive = lines[s].split('/')[1]\n            filename_pred_png = save_name + '/raw/' + date_drive + '_' + lines[s].split()[0].split('/')[-1].replace(\n                '.jpg', '.png')\n            filename_cmap_png = save_name + '/cmap/' + date_drive + '_' + lines[s].split()[0].split('/')[\n                -1].replace('.jpg', '.png')\n            filename_image_png = save_name + '/rgb/' + date_drive + '_' + lines[s].split()[0].split('/')[-1]\n        \n        elif args.dataset == 'kittipred':\n            filename_pred_png = save_name + '/raw/' + lines[s].split()[0].split('/')[-1].replace('.jpg', '.png')\n            filename_cmap_png = save_name + '/cmap/' + lines[s].split()[0].split('/')[-1].replace('.jpg', '.png')\n            filename_image_png = save_name + '/rgb/' + lines[s].split()[0].split('/')[-1]\n        \n#         else:\n#             scene_name = lines[s].split()[0].split('/')[0]\n#             print(f\"Processing line {s + 1}/{num_test_samples}: {lines[s]}\")\n#             filename_pred_png = save_name + '/raw/' + scene_name + '_' + lines[s].split()[0].split('/')[1].replace(\n#                 '.png', '.png')\n#             filename_cmap_png = save_name + '/cmap/' + scene_name + '_' + lines[s].split()[0].split('/rgb_')[1].replace(\n#                 '.png', '.png')\n#             filename_gt_png = save_name + '/gt/' + scene_name + '_' + lines[s].split()[0].split('/rgb_')[1].replace(\n#                 '.png', '_gt.png')\n#             filename_image_png = save_name + '/rgb/' + scene_name + '_' + lines[s].split()[0].split('/rgb_')[1]\n\n\n#         else:\n#             scene_name = lines[s].split()[0].split('/')[0]\n#             line_parts = lines[s].split()\n#             print(f\"Processing line {s + 1}/{num_test_samples}: {lines[s]}\")\n#             filename_pred_png = save_name + '/raw/' + scene_name + '_' + line_parts[0].replace('.tif', '_') + '.png'\n#             filename_cmap_png = save_name + '/cmap/' + scene_name + '_' + line_parts[0].replace('.tif', '_') + '.png'\n#             filename_gt_png = save_name + '/gt/' + scene_name + '_' + line_parts[0].replace('.tif', '_gt') + '.png'\n#             filename_image_png = save_name + '/rgb/' + scene_name + '_' + line_parts[0].replace('.tif', '_') + '.png'\n\n        else:\n            scene_name = lines[s].split()[0].split('/')[0]\n            line_parts = lines[s].split()\n            print(f\"Processing line {s + 1}/{num_test_samples}: {lines[s]}\")\n\n            # 分割两个文件的路径\n            rgb_file_path, height_file_path = line_parts[0], line_parts[1]\n\n            # 构建文件名\n            filename_pred_png = os.path.join(save_name, 'raw', os.path.basename(height_file_path).replace('.png', '_') + '.png')\n            filename_cmap_png = os.path.join(save_name, 'cmap', os.path.basename(height_file_path).replace('.png', '_') + '.png')\n            filename_gt_png = os.path.join(save_name, 'gt', os.path.basename(height_file_path).replace('.png', '_gt') + '.png')\n            filename_image_png = os.path.join(save_name, 'rgb', os.path.basename(height_file_path).replace('.png', '_') + '.png')\n\n\n\n\n        \n        rgb_path = os.path.join(args.data_path, './' + lines[s].split()[0])\n        image = cv2.imread(rgb_path)\n        \n        if args.dataset == 'nyu':\n            gt_path = os.path.join(args.data_path, './' + lines[s].split()[1])\n            gt = cv2.imread(gt_path, -1).astype(np.float32) / 1000.0  # Visualization purpose only\n            gt[gt == 0] = np.amax(gt)\n        \n        pred_depth = pred_depths[s]\n        \n        if args.dataset == 'kitti' or args.dataset == 'kittipred':\n            pred_depth_scaled = pred_depth * 256.0\n        else:\n            pred_depth_scaled = pred_depth * 1000.0\n        \n        pred_depth_scaled = pred_depth_scaled.astype(np.uint16)\n        cv2.imwrite(filename_pred_png, pred_depth_scaled, [cv2.IMWRITE_PNG_COMPRESSION, 0])\n        \n        \n        # 设置是否保存后三种影像--还保存\n        if args.save_viz:\n#             cv2.imwrite(filename_image_png, image[10:-1 - 9, 10:-1 - 9, :])\n            if args.dataset == 'nyu':\n#                 plt.imsave(filename_gt_png, (10 - gt) / 10, cmap='plasma')\n                pred_depth_cropped = pred_depth[10:-1 - 9, 10:-1 - 9]\n                plt.imsave(filename_cmap_png, (10 - pred_depth) / 10, cmap='plasma')\n            else:\n                plt.imsave(filename_cmap_png, np.log10(pred_depth), cmap='Greys')\n    \n    return\n\n\nif __name__ == '__main__':\n    test(args)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T02:33:38.481156Z","iopub.execute_input":"2023-11-28T02:33:38.481543Z","iopub.status.idle":"2023-11-28T02:33:38.494893Z","shell.execute_reply.started":"2023-11-28T02:33:38.481513Z","shell.execute_reply":"2023-11-28T02:33:38.493953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile /kaggle/working/VADepthNet/vadepthnet/test.py\n# 测试两张RS影像查看结果\nfrom __future__ import absolute_import, division, print_function\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nimport os, sys, errno\nimport argparse\nimport time\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\nfrom utils import post_process_depth, flip_lr\nfrom networks.vadepthnet import VADepthNet\n\nimport logging\n# 创建一个记录器实例\nlogger = logging.getLogger(__name__)\n\ndef convert_arg_line_to_args(arg_line):\n    for arg in arg_line.split():\n        if not arg.strip():\n            continue\n        yield arg\n\n\nparser = argparse.ArgumentParser(description='VADepthNet PyTorch implementation.', fromfile_prefix_chars='@')\nparser.convert_arg_line_to_args = convert_arg_line_to_args\n\n# --pretrain /kaggle/input/depthandmodels/swin_base_patch4_window12_384_22k.pth\nparser.add_argument('--pretrain', type=str,   help='path of pretrained encoder', default=None)\nparser.add_argument('--prior_mean', type=float, help='mean value for prior', default=3.54)\n\nparser.add_argument('--model_name', type=str, help='model name', default='vadepthnet')\n# parser.add_argument('--encoder', type=str, help='type of encoder, base07, large07', default='large07')\nparser.add_argument('--data_path', type=str, help='path to the data', default='/kaggle/working/')\nparser.add_argument('--filenames_file', type=str, help='path to the filenames text file', default='/kaggle/input/smallfile/osiedited.txt')\nparser.add_argument('--input_height', type=int, help='input height', default=480)\nparser.add_argument('--input_width', type=int, help='input width', default=640)\nparser.add_argument('--max_depth', type=float, help='maximum depth in estimation', default=10)\nparser.add_argument('--checkpoint_path', type=str, help='path to a specific checkpoint to load', default='/kaggle/input/depthandmodels/vadepthnet_nyu.pth')\nparser.add_argument('--dataset', type=str, help='dataset to train on', default='nyu')\n# 这个参数是将数据集裁剪为Kitti以及benchmark数据集的尺寸大小\nparser.add_argument('--do_kb_crop', help='if set, crop input images as kitti benchmark images', action='store_true')\n# 这个参数是用来设置是否需要保存彩色深度结果、原图、和GT图像，已注释后两者\nparser.add_argument('--save_viz', help='if set, save visulization of the outputs', action='store_true', default=True)\n\nif sys.argv.__len__() == 2:\n    arg_filename_with_prefix = '@' + sys.argv[1]\n    args = parser.parse_args([arg_filename_with_prefix])\nelse:\n    args = parser.parse_args()\n\nif args.dataset == 'kitti' or args.dataset == 'nyu':\n    from dataloaders.dataloader import NewDataLoader\nelif args.dataset == 'kittipred':\n    from dataloaders.dataloader_kittipred import NewDataLoader\n\nmodel_dir = os.path.dirname(args.checkpoint_path)\nsys.path.append(model_dir)\n\n\ndef get_num_lines(file_path):\n    f = open(file_path, 'r')\n    lines = f.readlines()\n    f.close()\n    return len(lines)\n\n\ndef test(params):\n    \"\"\"Test function.\"\"\"\n    args.mode = 'test'\n    dataloader = NewDataLoader(args, 'test')\n    \n#     model = NewCRFDepth(version='large07', inv_depth=False, max_depth=args.max_depth)\n#     model = VADepthNet(version='large07', inv_depth=False, max_depth=args.max_depth)\n    model = VADepthNet(pretrained=args.pretrain, max_depth=args.max_depth, prior_mean=args.prior_mean,\n                       img_size=(args.input_height, args.input_width))\n\n    model = torch.nn.DataParallel(model)\n    \n    checkpoint = torch.load(args.checkpoint_path)\n    model.load_state_dict(checkpoint['model'])\n    model.eval()\n    model.cuda()\n\n    num_params = sum([np.prod(p.size()) for p in model.parameters()])\n    print(\"Total number of parameters: {}\".format(num_params))\n\n    num_test_samples = get_num_lines(args.filenames_file)\n\n    with open(args.filenames_file) as f:\n        lines = f.readlines()\n\n#     print('now testing {} files with {}'.format(num_test_samples, args.checkpoint_path))\n    print('now testing {} files with {}'.format(args.checkpoint_path, num_test_samples))\n\n    pred_depths = []\n    start_time = time.time()\n    with torch.no_grad():\n        for _, sample in enumerate(tqdm(dataloader.data)):\n            image = Variable(sample['image'].cuda())\n            # Predict\n            depth_est = model(image)\n            post_process = True\n            if post_process:\n                image_flipped = flip_lr(image)\n                depth_est_flipped = model(image_flipped)\n                depth_est = post_process_depth(depth_est, depth_est_flipped)\n\n            pred_depth = depth_est.cpu().numpy().squeeze()\n\n            if args.do_kb_crop:\n                height, width = 352, 1216\n                top_margin = int(height - 352)\n                left_margin = int((width - 1216) / 2)\n                pred_depth_uncropped = np.zeros((height, width), dtype=np.float32)\n                pred_depth_uncropped[top_margin:top_margin + 352, left_margin:left_margin + 1216] = pred_depth\n                pred_depth = pred_depth_uncropped\n\n            pred_depths.append(pred_depth)\n\n    elapsed_time = time.time() - start_time\n    print('Elapesed time: %s' % str(elapsed_time))\n    print('Done.')\n    \n    save_name = '/kaggle/working/result_' + args.model_name\n    \n    print('Saving result pngs..')\n    if not os.path.exists(save_name):\n        try:\n            os.mkdir(save_name)\n            os.mkdir(save_name + '/raw')\n            os.mkdir(save_name + '/cmap')\n            os.mkdir(save_name + '/rgb')\n            os.mkdir(save_name + '/gt')\n        except OSError as e:\n            if e.errno != errno.EEXIST:\n                raise\n    \n    for s in tqdm(range(num_test_samples)):\n        if args.dataset == 'kitti':\n            date_drive = lines[s].split('/')[1]\n            filename_pred_png = save_name + '/raw/' + date_drive + '_' + lines[s].split()[0].split('/')[-1].replace(\n                '.jpg', '.png')\n            filename_cmap_png = save_name + '/cmap/' + date_drive + '_' + lines[s].split()[0].split('/')[\n                -1].replace('.jpg', '.png')\n            filename_image_png = save_name + '/rgb/' + date_drive + '_' + lines[s].split()[0].split('/')[-1]\n        \n        elif args.dataset == 'kittipred':\n            filename_pred_png = save_name + '/raw/' + lines[s].split()[0].split('/')[-1].replace('.jpg', '.png')\n            filename_cmap_png = save_name + '/cmap/' + lines[s].split()[0].split('/')[-1].replace('.jpg', '.png')\n            filename_image_png = save_name + '/rgb/' + lines[s].split()[0].split('/')[-1]\n        \n#         else:\n#             scene_name = lines[s].split()[0].split('/')[0]\n#             print(f\"Processing line {s + 1}/{num_test_samples}: {lines[s]}\")\n#             filename_pred_png = save_name + '/raw/' + scene_name + '_' + lines[s].split()[0].split('/')[1].replace(\n#                 '.png', '.png')\n#             filename_cmap_png = save_name + '/cmap/' + scene_name + '_' + lines[s].split()[0].split('/rgb_')[1].replace(\n#                 '.png', '.png')\n#             filename_gt_png = save_name + '/gt/' + scene_name + '_' + lines[s].split()[0].split('/rgb_')[1].replace(\n#                 '.png', '_gt.png')\n#             filename_image_png = save_name + '/rgb/' + scene_name + '_' + lines[s].split()[0].split('/rgb_')[1]\n\n\n        else:\n            scene_name = lines[s].split()[0].split('/')[0]\n            line_parts = lines[s].split()\n            print(f\"Processing line {s + 1}/{num_test_samples}: {lines[s]}\")\n            filename_pred_png = save_name + '/raw/' + scene_name + '_' + line_parts[0].replace('.png', '_') + '.png'\n            filename_cmap_png = save_name + '/cmap/' + scene_name + '_' + line_parts[0].replace('.png', '_') + '.png'\n            filename_gt_png = save_name + '/gt/' + scene_name + '_' + line_parts[0].replace('.png', '_gt') + '.png'\n            filename_image_png = save_name + '/rgb/' + scene_name + '_' + line_parts[0].replace('.png', '_') + '.png'\n\n\n\n        \n        rgb_path = os.path.join(args.data_path, './' + lines[s].split()[0])\n        image = cv2.imread(rgb_path)\n        \n        if args.dataset == 'nyu':\n            gt_path = os.path.join(args.data_path, './' + lines[s].split()[1])\n            gt = cv2.imread(gt_path, -1).astype(np.float32) / 1000.0  # Visualization purpose only\n            gt[gt == 0] = np.amax(gt)\n        \n        pred_depth = pred_depths[s]\n        \n        if args.dataset == 'kitti' or args.dataset == 'kittipred':\n            pred_depth_scaled = pred_depth * 256.0\n        else:\n            pred_depth_scaled = pred_depth * 1000.0\n        \n        pred_depth_scaled = pred_depth_scaled.astype(np.uint16)\n        cv2.imwrite(filename_pred_png, pred_depth_scaled, [cv2.IMWRITE_PNG_COMPRESSION, 0])\n        \n        \n        # 设置是否保存后三种影像--还保存\n        if args.save_viz:\n#             cv2.imwrite(filename_image_png, image[10:-1 - 9, 10:-1 - 9, :])\n            if args.dataset == 'nyu':\n#                 plt.imsave(filename_gt_png, (10 - gt) / 10, cmap='plasma')\n                pred_depth_cropped = pred_depth[10:-1 - 9, 10:-1 - 9]\n                plt.imsave(filename_cmap_png, (10 - pred_depth) / 10, cmap='plasma')\n            else:\n                plt.imsave(filename_cmap_png, np.log10(pred_depth), cmap='Greys')\n    \n    return\n\n\nif __name__ == '__main__':\n    test(args)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile /kaggle/working/VADepthNet/vadepthnet/test.py\nfrom __future__ import absolute_import, division, print_function\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nimport os, sys, errno\nimport argparse\nimport time\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\nfrom utils import post_process_depth, flip_lr\nfrom networks.vadepthnet import VADepthNet\n\nimport logging\n# 创建一个记录器实例\nlogger = logging.getLogger(__name__)\n\ndef convert_arg_line_to_args(arg_line):\n    for arg in arg_line.split():\n        if not arg.strip():\n            continue\n        yield arg\n\n\nparser = argparse.ArgumentParser(description='VADepthNet PyTorch implementation.', fromfile_prefix_chars='@')\nparser.convert_arg_line_to_args = convert_arg_line_to_args\n\n# --pretrain /kaggle/input/depthandmodels/swin_base_patch4_window12_384_22k.pth\nparser.add_argument('--pretrain', type=str,   help='path of pretrained encoder', default=None)\nparser.add_argument('--prior_mean', type=float, help='mean value for prior', default=0.0)\n\nparser.add_argument('--model_name', type=str, help='model name', default='vadepthnet')\n# parser.add_argument('--encoder', type=str, help='type of encoder, base07, large07', default='large07')\nparser.add_argument('--data_path', type=str, help='path to the data', default='/kaggle/input/image-depth-estimation/data/nyu2_test')\nparser.add_argument('--filenames_file', type=str, help='path to the filenames text file', default='/kaggle/working/VADepthNet/data_splits/vad_nyu2_test.txt')\nparser.add_argument('--input_height', type=int, help='input height', default=480)\nparser.add_argument('--input_width', type=int, help='input width', default=640)\nparser.add_argument('--max_depth', type=float, help='maximum depth in estimation', default=10)\nparser.add_argument('--checkpoint_path', type=str, help='path to a specific checkpoint to load', default='/kaggle/input/depthandmodels/vadepthnet_nyu.pth')\nparser.add_argument('--dataset', type=str, help='dataset to train on', default='nyu')\n# 这个参数是将数据集裁剪为Kitti以及benchmark数据集的尺寸大小\nparser.add_argument('--do_kb_crop', help='if set, crop input images as kitti benchmark images', action='store_true')\n# 这个参数是用来设置是否需要保存彩色深度结果、原图、和GT图像，已注释后两者\nparser.add_argument('--save_viz', help='if set, save visulization of the outputs', action='store_true', default=True)\n\nif sys.argv.__len__() == 2:\n    arg_filename_with_prefix = '@' + sys.argv[1]\n    args = parser.parse_args([arg_filename_with_prefix])\nelse:\n    args = parser.parse_args()\n\nif args.dataset == 'kitti' or args.dataset == 'nyu':\n    from dataloaders.dataloader import NewDataLoader\nelif args.dataset == 'kittipred':\n    from dataloaders.dataloader_kittipred import NewDataLoader\n\nmodel_dir = os.path.dirname(args.checkpoint_path)\nsys.path.append(model_dir)\n\n\ndef get_num_lines(file_path):\n    f = open(file_path, 'r')\n    lines = f.readlines()\n    f.close()\n    return len(lines)\n\n\ndef test(params):\n    \"\"\"Test function.\"\"\"\n    args.mode = 'test'\n    dataloader = NewDataLoader(args, 'test')\n    \n#     model = NewCRFDepth(version='large07', inv_depth=False, max_depth=args.max_depth)\n#     model = VADepthNet(version='large07', inv_depth=False, max_depth=args.max_depth)\n    model = VADepthNet(pretrained=args.pretrain, max_depth=args.max_depth, prior_mean=args.prior_mean,\n                       img_size=(args.input_height, args.input_width))\n\n    model = torch.nn.DataParallel(model)\n    \n    checkpoint = torch.load(args.checkpoint_path)\n    model.load_state_dict(checkpoint['model'])\n    model.eval()\n    model.cuda()\n\n    num_params = sum([np.prod(p.size()) for p in model.parameters()])\n    print(\"Total number of parameters: {}\".format(num_params))\n\n    num_test_samples = get_num_lines(args.filenames_file)\n\n    with open(args.filenames_file) as f:\n        lines = f.readlines()\n\n#     print('now testing {} files with {}'.format(num_test_samples, args.checkpoint_path))\n    print('now testing {} files with {}'.format(args.checkpoint_path, num_test_samples))\n\n    pred_depths = []\n    start_time = time.time()\n    with torch.no_grad():\n        for _, sample in enumerate(tqdm(dataloader.data)):\n            image = Variable(sample['image'].cuda())\n            # Predict\n            depth_est = model(image)\n            post_process = True\n            if post_process:\n                image_flipped = flip_lr(image)\n                depth_est_flipped = model(image_flipped)\n                depth_est = post_process_depth(depth_est, depth_est_flipped)\n\n            pred_depth = depth_est.cpu().numpy().squeeze()\n\n            if args.do_kb_crop:\n                height, width = 352, 1216\n                top_margin = int(height - 352)\n                left_margin = int((width - 1216) / 2)\n                pred_depth_uncropped = np.zeros((height, width), dtype=np.float32)\n                pred_depth_uncropped[top_margin:top_margin + 352, left_margin:left_margin + 1216] = pred_depth\n                pred_depth = pred_depth_uncropped\n\n            pred_depths.append(pred_depth)\n\n    elapsed_time = time.time() - start_time\n    print('Elapesed time: %s' % str(elapsed_time))\n    print('Done.')\n    \n    save_name = '/kaggle/working/result_' + args.model_name\n    \n    print('Saving result pngs..')\n    if not os.path.exists(save_name):\n        try:\n            os.mkdir(save_name)\n            os.mkdir(save_name + '/raw')\n            os.mkdir(save_name + '/cmap')\n            os.mkdir(save_name + '/rgb')\n            os.mkdir(save_name + '/gt')\n        except OSError as e:\n            if e.errno != errno.EEXIST:\n                raise\n    \n    for s in tqdm(range(num_test_samples)):\n        if args.dataset == 'kitti':\n            date_drive = lines[s].split('/')[1]\n            filename_pred_png = save_name + '/raw/' + date_drive + '_' + lines[s].split()[0].split('/')[-1].replace(\n                '.jpg', '.png')\n            filename_cmap_png = save_name + '/cmap/' + date_drive + '_' + lines[s].split()[0].split('/')[\n                -1].replace('.jpg', '.png')\n            filename_image_png = save_name + '/rgb/' + date_drive + '_' + lines[s].split()[0].split('/')[-1]\n        \n        elif args.dataset == 'kittipred':\n            filename_pred_png = save_name + '/raw/' + lines[s].split()[0].split('/')[-1].replace('.jpg', '.png')\n            filename_cmap_png = save_name + '/cmap/' + lines[s].split()[0].split('/')[-1].replace('.jpg', '.png')\n            filename_image_png = save_name + '/rgb/' + lines[s].split()[0].split('/')[-1]\n        \n#         else:\n#             scene_name = lines[s].split()[0].split('/')[0]\n#             print(f\"Processing line {s + 1}/{num_test_samples}: {lines[s]}\")\n#             filename_pred_png = save_name + '/raw/' + scene_name + '_' + lines[s].split()[0].split('/')[1].replace(\n#                 '.png', '.png')\n#             filename_cmap_png = save_name + '/cmap/' + scene_name + '_' + lines[s].split()[0].split('/rgb_')[1].replace(\n#                 '.png', '.png')\n#             filename_gt_png = save_name + '/gt/' + scene_name + '_' + lines[s].split()[0].split('/rgb_')[1].replace(\n#                 '.png', '_gt.png')\n#             filename_image_png = save_name + '/rgb/' + scene_name + '_' + lines[s].split()[0].split('/rgb_')[1]\n\n\n        else:\n            scene_name = lines[s].split()[0].split('/')[0]\n            line_parts = lines[s].split()\n            print(f\"Processing line {s + 1}/{num_test_samples}: {lines[s]}\")\n            filename_pred_png = save_name + '/raw/' + scene_name + '_' + line_parts[0].replace('.png', '_') + '.png'\n            filename_cmap_png = save_name + '/cmap/' + scene_name + '_' + line_parts[0].replace('.png', '_') + '.png'\n            filename_gt_png = save_name + '/gt/' + scene_name + '_' + line_parts[0].replace('.png', '_gt') + '.png'\n            filename_image_png = save_name + '/rgb/' + scene_name + '_' + line_parts[0].replace('.png', '_') + '.png'\n\n\n\n        \n        rgb_path = os.path.join(args.data_path, './' + lines[s].split()[0])\n        image = cv2.imread(rgb_path)\n        \n        if args.dataset == 'nyu':\n            gt_path = os.path.join(args.data_path, './' + lines[s].split()[1])\n            gt = cv2.imread(gt_path, -1).astype(np.float32) / 1000.0  # Visualization purpose only\n            gt[gt == 0] = np.amax(gt)\n        \n        pred_depth = pred_depths[s]\n        \n        if args.dataset == 'kitti' or args.dataset == 'kittipred':\n            pred_depth_scaled = pred_depth * 256.0\n        else:\n            pred_depth_scaled = pred_depth * 1000.0\n        \n        pred_depth_scaled = pred_depth_scaled.astype(np.uint16)\n        cv2.imwrite(filename_pred_png, pred_depth_scaled, [cv2.IMWRITE_PNG_COMPRESSION, 0])\n        \n        \n        # 设置是否保存后三种影像--还保存\n        if args.save_viz:\n#             cv2.imwrite(filename_image_png, image[10:-1 - 9, 10:-1 - 9, :])\n            if args.dataset == 'nyu':\n#                 plt.imsave(filename_gt_png, (10 - gt) / 10, cmap='plasma')\n                pred_depth_cropped = pred_depth[10:-1 - 9, 10:-1 - 9]\n                plt.imsave(filename_cmap_png, (10 - pred_depth) / 10, cmap='plasma')\n            else:\n                plt.imsave(filename_cmap_png, np.log10(pred_depth), cmap='Greys')\n    \n    return\n\n\nif __name__ == '__main__':\n    test(args)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 从Ne中找到的test.py原始代码\nfrom __future__ import absolute_import, division, print_function\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nimport os, sys, errno\nimport argparse\nimport time\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\nfrom utils import post_process_depth, flip_lr\nfrom networks.NewCRFDepth import NewCRFDepth\n\n\ndef convert_arg_line_to_args(arg_line):\n    for arg in arg_line.split():\n        if not arg.strip():\n            continue\n        yield arg\n\n\nparser = argparse.ArgumentParser(description='NeWCRFs PyTorch implementation.', fromfile_prefix_chars='@')\nparser.convert_arg_line_to_args = convert_arg_line_to_args\n\nparser.add_argument('--model_name', type=str, help='model name', default='newcrfs')\nparser.add_argument('--encoder', type=str, help='type of encoder, base07, large07', default='large07')\nparser.add_argument('--data_path', type=str, help='path to the data', required=True)\nparser.add_argument('--filenames_file', type=str, help='path to the filenames text file', required=True)\nparser.add_argument('--input_height', type=int, help='input height', default=480)\nparser.add_argument('--input_width', type=int, help='input width', default=640)\nparser.add_argument('--max_depth', type=float, help='maximum depth in estimation', default=10)\nparser.add_argument('--checkpoint_path', type=str, help='path to a specific checkpoint to load', default='')\nparser.add_argument('--dataset', type=str, help='dataset to train on', default='nyu')\nparser.add_argument('--do_kb_crop', help='if set, crop input images as kitti benchmark images', action='store_true')\nparser.add_argument('--save_viz', help='if set, save visulization of the outputs', action='store_true')\n\nif sys.argv.__len__() == 2:\n    arg_filename_with_prefix = '@' + sys.argv[1]\n    args = parser.parse_args([arg_filename_with_prefix])\nelse:\n    args = parser.parse_args()\n\nif args.dataset == 'kitti' or args.dataset == 'nyu':\n    from dataloaders.dataloader import NewDataLoader\nelif args.dataset == 'kittipred':\n    from dataloaders.dataloader_kittipred import NewDataLoader\n\nmodel_dir = os.path.dirname(args.checkpoint_path)\nsys.path.append(model_dir)\n\n\ndef get_num_lines(file_path):\n    f = open(file_path, 'r')\n    lines = f.readlines()\n    f.close()\n    return len(lines)\n\n\ndef test(params):\n    \"\"\"Test function.\"\"\"\n    args.mode = 'test'\n    dataloader = NewDataLoader(args, 'test')\n    \n    model = NewCRFDepth(version='large07', inv_depth=False, max_depth=args.max_depth)\n    model = torch.nn.DataParallel(model)\n    \n    checkpoint = torch.load(args.checkpoint_path)\n    model.load_state_dict(checkpoint['model'])\n    model.eval()\n    model.cuda()\n\n    num_params = sum([np.prod(p.size()) for p in model.parameters()])\n    print(\"Total number of parameters: {}\".format(num_params))\n\n    num_test_samples = get_num_lines(args.filenames_file)\n\n    with open(args.filenames_file) as f:\n        lines = f.readlines()\n\n    print('now testing {} files with {}'.format(num_test_samples, args.checkpoint_path))\n\n    pred_depths = []\n    start_time = time.time()\n    with torch.no_grad():\n        for _, sample in enumerate(tqdm(dataloader.data)):\n            image = Variable(sample['image'].cuda())\n            # Predict\n            depth_est = model(image)\n            post_process = True\n            if post_process:\n                image_flipped = flip_lr(image)\n                depth_est_flipped = model(image_flipped)\n                depth_est = post_process_depth(depth_est, depth_est_flipped)\n\n            pred_depth = depth_est.cpu().numpy().squeeze()\n\n            if args.do_kb_crop:\n                height, width = 352, 1216\n                top_margin = int(height - 352)\n                left_margin = int((width - 1216) / 2)\n                pred_depth_uncropped = np.zeros((height, width), dtype=np.float32)\n                pred_depth_uncropped[top_margin:top_margin + 352, left_margin:left_margin + 1216] = pred_depth\n                pred_depth = pred_depth_uncropped\n\n            pred_depths.append(pred_depth)\n\n    elapsed_time = time.time() - start_time\n    print('Elapesed time: %s' % str(elapsed_time))\n    print('Done.')\n    \n    save_name = 'models/result_' + args.model_name\n    \n    print('Saving result pngs..')\n    if not os.path.exists(save_name):\n        try:\n            os.mkdir(save_name)\n            os.mkdir(save_name + '/raw')\n            os.mkdir(save_name + '/cmap')\n            os.mkdir(save_name + '/rgb')\n            os.mkdir(save_name + '/gt')\n        except OSError as e:\n            if e.errno != errno.EEXIST:\n                raise\n    \n    for s in tqdm(range(num_test_samples)):\n        if args.dataset == 'kitti':\n            date_drive = lines[s].split('/')[1]\n            filename_pred_png = save_name + '/raw/' + date_drive + '_' + lines[s].split()[0].split('/')[-1].replace(\n                '.jpg', '.png')\n            filename_cmap_png = save_name + '/cmap/' + date_drive + '_' + lines[s].split()[0].split('/')[\n                -1].replace('.jpg', '.png')\n            filename_image_png = save_name + '/rgb/' + date_drive + '_' + lines[s].split()[0].split('/')[-1]\n        elif args.dataset == 'kittipred':\n            filename_pred_png = save_name + '/raw/' + lines[s].split()[0].split('/')[-1].replace('.jpg', '.png')\n            filename_cmap_png = save_name + '/cmap/' + lines[s].split()[0].split('/')[-1].replace('.jpg', '.png')\n            filename_image_png = save_name + '/rgb/' + lines[s].split()[0].split('/')[-1]\n        else:\n            scene_name = lines[s].split()[0].split('/')[0]\n            filename_pred_png = save_name + '/raw/' + scene_name + '_' + lines[s].split()[0].split('/')[1].replace(\n                '.jpg', '.png')\n            filename_cmap_png = save_name + '/cmap/' + scene_name + '_' + lines[s].split()[0].split('/rgb_')[1].replace(\n                '.jpg', '.png')\n            filename_gt_png = save_name + '/gt/' + scene_name + '_' + lines[s].split()[0].split('/rgb_')[1].replace(\n                '.jpg', '_gt.png')\n            filename_image_png = save_name + '/rgb/' + scene_name + '_' + lines[s].split()[0].split('/rgb_')[1]\n        \n        rgb_path = os.path.join(args.data_path, './' + lines[s].split()[0])\n        image = cv2.imread(rgb_path)\n        if args.dataset == 'nyu':\n            gt_path = os.path.join(args.data_path, './' + lines[s].split()[1])\n            gt = cv2.imread(gt_path, -1).astype(np.float32) / 1000.0  # Visualization purpose only\n            gt[gt == 0] = np.amax(gt)\n        \n        pred_depth = pred_depths[s]\n        \n        if args.dataset == 'kitti' or args.dataset == 'kittipred':\n            pred_depth_scaled = pred_depth * 256.0\n        else:\n            pred_depth_scaled = pred_depth * 1000.0\n        \n        pred_depth_scaled = pred_depth_scaled.astype(np.uint16)\n        cv2.imwrite(filename_pred_png, pred_depth_scaled, [cv2.IMWRITE_PNG_COMPRESSION, 0])\n        \n        if args.save_viz:\n            cv2.imwrite(filename_image_png, image[10:-1 - 9, 10:-1 - 9, :])\n            if args.dataset == 'nyu':\n                plt.imsave(filename_gt_png, (10 - gt) / 10, cmap='plasma')\n                pred_depth_cropped = pred_depth[10:-1 - 9, 10:-1 - 9]\n                plt.imsave(filename_cmap_png, (10 - pred_depth) / 10, cmap='plasma')\n            else:\n                plt.imsave(filename_cmap_png, np.log10(pred_depth), cmap='Greys')\n    \n    return\n\n\nif __name__ == '__main__':\n    test(args)","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n/kaggle/working/VADepthNet/vadepthnet/networks/vadepthnet.py","metadata":{}},{"cell_type":"code","source":"%%writefile /kaggle/working/VADepthNet/vadepthnet/networks/vadepthnet.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .swin_transformer import SwinTransformer\nfrom .loss import VarLoss, SILogLoss\n########################################################################################################################\n\nclass DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels, mid_channels=None):\n        super().__init__()\n        \n        if not mid_channels:\n            mid_channels = out_channels\n        \n        self.conv1 =  nn.Sequential(\n            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, groups=4),\n            #nn.BatchNorm2d(mid_channels),\n            nn.InstanceNorm2d(mid_channels),\n            #nn.ReLU(inplace=True),\n            nn.LeakyReLU(),\n        )\n        self.conv2 = nn.Sequential(\n            #ModulatedDeformConvPack(mid_channels, out_channels, kernel_size=3, padding=1),\n            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n            #nn.BatchNorm2d(out_channels),\n            nn.InstanceNorm2d(out_channels),\n            #nn.ReLU(inplace=True),\n            nn.LeakyReLU(),\n        )\n\n        self.bt = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n\n\n    def forward(self, x):\n        skip = self.bt(x)\n\n        x = self.channel_shuffle(x, 4)\n\n        x = self.conv1(x)\n\n        x = self.conv2(x)\n\n        return x + skip\n\n    def channel_shuffle(self, x, groups):\n        batchsize, num_channels, height, width = x.shape\n\n        channels_per_group = num_channels // groups\n        # reshape\n        x = x.view(batchsize, groups, channels_per_group, height, width)\n\n        x = torch.transpose(x, 1, 2).contiguous()\n\n        # flatten\n        x = x.view(batchsize, -1, height, width)\n\n        return x\n\n\n\nclass Up(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n\n        self.up = nn.Upsample(\n            scale_factor=2, mode='bilinear', align_corners=True)\n        self.conv = DoubleConv(\n            in_channels, out_channels, in_channels)\n\n    def forward(self, x1, x2=None):\n        x1 = self.up(x1)\n        # input is CHW\n        if x2 is not None:\n            diffY = x2.size()[2] - x1.size()[2]\n            diffX = x2.size()[3] - x1.size()[3]\n            if diffX > 0 or diffY > 0:\n                x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n                                diffY // 2, diffY - diffY // 2])\n            x = torch.cat([x2, x1], dim=1)\n        else:\n            x = x1\n        return self.conv(x)\n\nclass OutConv(nn.Module):\n    def __init__(self, in_channels, out_channels, prior_mean = 1.54):\n        super(OutConv, self).__init__()\n\n        self.prior_mean = prior_mean\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        return torch.exp(self.conv(x) + self.prior_mean)\n\n\nclass VarLayer(nn.Module):\n    def __init__(self, in_channels, h, w):\n        super(VarLayer, self).__init__()\n\n        self.gr = 16\n\n        self.grad = nn.Sequential(\n                nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1),\n                #nn.BatchNorm2d(in_channels),\n                nn.LeakyReLU(),\n                nn.Conv2d(in_channels, 4*self.gr, kernel_size=3, padding=1))\n\n        self.att = nn.Sequential(\n                nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1),\n                #nn.BatchNorm2d(in_channels),\n                nn.LeakyReLU(),\n                nn.Conv2d(in_channels, 4*self.gr, kernel_size=3, padding=1),\n                nn.Sigmoid())\n\n\n        num = h * w\n\n        a = torch.zeros(num, 4, num, dtype=torch.float16)\n\n        for i in range(num):\n\n            #a[i, 0, i] = 1.0\n            #if i + 1 < num:\n            if (i+1) % w != 0 and (i+1) < num:\n                a[i, 0, i] = 1.0\n                a[i, 0, i+1] = -1.0\n\n            #a[i, 1, i] = 1.0\n            if i + w < num:\n                a[i, 1, i] = 1.0\n                a[i, 1, i+w] = -1.0\n\n            if (i+2) % w != 0 and (i+2) < num:\n                a[i, 2, i] = 1.0\n                a[i, 2, i+2] = -1.0\n\n            if i + w + w < num:\n                a[i, 3, i] = 1.0\n                a[i, 3, i+w+w] = -1.0\n\n        a[-1, 0, -1] = 1.0\n        a[-1, 1, -1] = 1.0\n\n        a[-1, 2, -1] = 1.0\n        a[-1, 3, -1] = 1.0\n\n        self.register_buffer('a', a.unsqueeze(0))\n\n        self.ins = nn.GroupNorm(1, self.gr)\n\n        self.se = nn.Sequential(\n                nn.AdaptiveAvgPool2d((1, 1)),\n                nn.Conv2d(in_channels, in_channels//2, kernel_size=1, padding=0),\n                nn.LeakyReLU(),\n                nn.Conv2d(in_channels//2, self.gr, kernel_size=1, padding=0),\n                nn.Sigmoid())\n\n        self.post = nn.Sequential(\n                nn.Conv2d(self.gr, 8*self.gr, kernel_size=3, padding=1))\n\n    def forward(self, x):\n        skip = x.clone()\n        att = self.att(x)\n        grad = self.grad(x)\n\n\n        se = self.se(x)\n\n        n, c, h, w = x.shape\n\n        att = att.reshape(n*self.gr, 4, h*w, 1).permute(0, 2, 1, 3)\n        grad = grad.reshape(n*self.gr, 4, h*w, 1).permute(0, 2, 1, 3)\n\n        A = self.a * att\n        B = grad * att\n\n        A = A.reshape(n*self.gr, h*w*4, h*w)\n        B = B.reshape(n*self.gr, h*w*4, 1)\n\n        AT = A.permute(0, 2, 1)\n\n        ATA = torch.bmm(AT, A)\n        ATB = torch.bmm(AT, B)\n\n        jitter = torch.eye(n=h*w, dtype=x.dtype, device=x.device).unsqueeze(0) * 1e-12\n\n#         x, _ = torch.solve(ATB, ATA+jitter)\n#         2023年11月20日22:39:21 报错修改\n        x = torch.linalg.solve(ATA + jitter, ATB)\n\n        x = x.reshape(n, self.gr, h, w)\n\n        x = self.ins(x)\n\n        x = se * x\n\n        x = self.post(x)\n\n        return x\n\n\n\nclass Refine(nn.Module):\n    def __init__(self, c1, c2):\n        super(Refine, self).__init__()\n\n        s = c1 + c2\n        self.fw = nn.Sequential(\n                nn.Conv2d(s, s, kernel_size=3, padding=1),\n                nn.LeakyReLU(),\n                nn.Conv2d(s, c1, kernel_size=3, padding=1))\n\n        self.dw = nn.Sequential(\n                nn.Conv2d(s, s, kernel_size=3, padding=1),\n                nn.LeakyReLU(),\n                nn.Conv2d(s, c2, kernel_size=3, padding=1))\n\n    def forward(self, feat, depth):\n        cc = torch.cat([feat, depth], 1)\n        feat_new = self.fw(cc)\n        depth_new = self.dw(cc)\n        return feat_new, depth_new\n\n\nclass MetricLayer(nn.Module):\n    def __init__(self, c):\n        super(MetricLayer, self).__init__()\n\n        self.ln = nn.Sequential(\n                nn.Linear(c, c//4),\n                nn.LeakyReLU(),\n                nn.Linear(c//4, 2))\n\n    def forward(self, x):\n\n        x = x.squeeze(-1).squeeze(-1)\n        x = self.ln(x)\n        x = x.unsqueeze(-1).unsqueeze(-1)\n\n        return x\n\n\nclass VADepthNet(nn.Module):\n    def __init__(self, pretrained=None, max_depth=65.0, prior_mean=2.58, si_lambda=0.85, img_size=(480, 480)):\n        super().__init__()\n\n        self.prior_mean = prior_mean\n        self.SI_loss_lambda = si_lambda\n        self.max_depth = max_depth\n\n        pretrain_img_size = img_size\n        patch_size = (4, 4)\n        in_chans = 3\n        embed_dim = 192\n        depths = [2, 2, 18, 2]\n        num_heads = [6, 12, 24, 48]\n        window_size = 12\n\n        backbone_cfg = dict(\n            pretrain_img_size=pretrain_img_size,\n            patch_size=patch_size,\n            embed_dim=embed_dim,\n            depths=depths,\n            num_heads=num_heads,\n            window_size=window_size,\n            ape=True,\n            drop_rate=0.\n        )\n\n\n        self.backbone = SwinTransformer(**backbone_cfg)\n        \n        self.backbone.init_weights(pretrained=pretrained)\n\n        self.up_4 = Up(1536 + 768, 512)\n        self.up_3 = Up(512 + 384, 256)\n        self.up_2 = Up(256 + 192, 64)\n\n        self.outc = OutConv(128, 1, self.prior_mean)\n\n        self.vlayer = VarLayer(512, img_size[0]//16, img_size[1]//16)\n\n        self.ref_4 = Refine(512, 128)\n        self.ref_3 = Refine(256, 128)\n        self.ref_2 = Refine(64, 128)\n\n        self.var_loss = VarLoss(128, 512)\n        self.si_loss = SILogLoss(self.SI_loss_lambda, self.max_depth)\n\n        self.mlayer = nn.Sequential(\n                nn.AdaptiveMaxPool2d((1,1)),\n                MetricLayer(1536))\n\n    def forward(self, x, gts=None):\n\n        x2, x3, x4, x5 = self.backbone(x)\n\n        outs = {}\n\n        metric = self.mlayer(x5)\n\n        x = self.up_4(x5, x4)\n        \n        d = self.vlayer(x)\n\n        if self.training:\n            var_loss = self.var_loss(x, d, gts)\n\n\n        x, d  = self.ref_4(x, d)\n\n        d_u4 = F.interpolate(d, scale_factor=16, mode='bilinear', align_corners=True)\n\n        x = self.up_3(x, x3)\n\n        x, d = self.ref_3(x, F.interpolate(d, scale_factor=2, mode='bilinear', align_corners=True))\n\n        d_u3 = F.interpolate(d, scale_factor=8, mode='bilinear', align_corners=True)\n\n        x = self.up_2(x, x2)\n\n        x, d = self.ref_2(x, F.interpolate(d, scale_factor=2, mode='bilinear', align_corners=True))\n\n        d_u2 = F.interpolate(d, scale_factor=4, mode='bilinear', align_corners=True)\n\n        d = d_u2 + d_u3 + d_u4\n\n        d = torch.sigmoid(metric[:, 0:1]) * (self.outc(d) + torch.exp(metric[:, 1:2]))\n\n        outs['scale_1'] = d\n\n        if self.training:\n            si_loss = self.si_loss(outs, gts)\n            return outs['scale_1'], var_loss + si_loss\n        else:\n            return outs['scale_1']","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# /kaggle/working/VADepthNet/vadepthnet/networks/vadepthnet.py原始代码\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .swin_transformer import SwinTransformer\nfrom .loss import VarLoss, SILogLoss\n########################################################################################################################\n\nclass DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels, mid_channels=None):\n        super().__init__()\n        \n        if not mid_channels:\n            mid_channels = out_channels\n        \n        self.conv1 =  nn.Sequential(\n            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, groups=4),\n            #nn.BatchNorm2d(mid_channels),\n            nn.InstanceNorm2d(mid_channels),\n            #nn.ReLU(inplace=True),\n            nn.LeakyReLU(),\n        )\n        self.conv2 = nn.Sequential(\n            #ModulatedDeformConvPack(mid_channels, out_channels, kernel_size=3, padding=1),\n            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n            #nn.BatchNorm2d(out_channels),\n            nn.InstanceNorm2d(out_channels),\n            #nn.ReLU(inplace=True),\n            nn.LeakyReLU(),\n        )\n\n        self.bt = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n\n\n    def forward(self, x):\n        skip = self.bt(x)\n\n        x = self.channel_shuffle(x, 4)\n\n        x = self.conv1(x)\n\n        x = self.conv2(x)\n\n        return x + skip\n\n    def channel_shuffle(self, x, groups):\n        batchsize, num_channels, height, width = x.shape\n\n        channels_per_group = num_channels // groups\n        # reshape\n        x = x.view(batchsize, groups, channels_per_group, height, width)\n\n        x = torch.transpose(x, 1, 2).contiguous()\n\n        # flatten\n        x = x.view(batchsize, -1, height, width)\n\n        return x\n\n\n\nclass Up(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n\n        self.up = nn.Upsample(\n            scale_factor=2, mode='bilinear', align_corners=True)\n        self.conv = DoubleConv(\n            in_channels, out_channels, in_channels)\n\n    def forward(self, x1, x2=None):\n        x1 = self.up(x1)\n        # input is CHW\n        if x2 is not None:\n            diffY = x2.size()[2] - x1.size()[2]\n            diffX = x2.size()[3] - x1.size()[3]\n            if diffX > 0 or diffY > 0:\n                x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n                                diffY // 2, diffY - diffY // 2])\n            x = torch.cat([x2, x1], dim=1)\n        else:\n            x = x1\n        return self.conv(x)\n\nclass OutConv(nn.Module):\n    def __init__(self, in_channels, out_channels, prior_mean = 1.54):\n        super(OutConv, self).__init__()\n\n        self.prior_mean = prior_mean\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        return torch.exp(self.conv(x) + self.prior_mean)\n\n\nclass VarLayer(nn.Module):\n    def __init__(self, in_channels, h, w):\n        super(VarLayer, self).__init__()\n\n        self.gr = 16\n\n        self.grad = nn.Sequential(\n                nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1),\n                #nn.BatchNorm2d(in_channels),\n                nn.LeakyReLU(),\n                nn.Conv2d(in_channels, 4*self.gr, kernel_size=3, padding=1))\n\n        self.att = nn.Sequential(\n                nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1),\n                #nn.BatchNorm2d(in_channels),\n                nn.LeakyReLU(),\n                nn.Conv2d(in_channels, 4*self.gr, kernel_size=3, padding=1),\n                nn.Sigmoid())\n\n\n        num = h * w\n\n        a = torch.zeros(num, 4, num, dtype=torch.float16)\n\n        for i in range(num):\n\n            #a[i, 0, i] = 1.0\n            #if i + 1 < num:\n            if (i+1) % w != 0 and (i+1) < num:\n                a[i, 0, i] = 1.0\n                a[i, 0, i+1] = -1.0\n\n            #a[i, 1, i] = 1.0\n            if i + w < num:\n                a[i, 1, i] = 1.0\n                a[i, 1, i+w] = -1.0\n\n            if (i+2) % w != 0 and (i+2) < num:\n                a[i, 2, i] = 1.0\n                a[i, 2, i+2] = -1.0\n\n            if i + w + w < num:\n                a[i, 3, i] = 1.0\n                a[i, 3, i+w+w] = -1.0\n\n        a[-1, 0, -1] = 1.0\n        a[-1, 1, -1] = 1.0\n\n        a[-1, 2, -1] = 1.0\n        a[-1, 3, -1] = 1.0\n\n        self.register_buffer('a', a.unsqueeze(0))\n\n        self.ins = nn.GroupNorm(1, self.gr)\n\n        self.se = nn.Sequential(\n                nn.AdaptiveAvgPool2d((1, 1)),\n                nn.Conv2d(in_channels, in_channels//2, kernel_size=1, padding=0),\n                nn.LeakyReLU(),\n                nn.Conv2d(in_channels//2, self.gr, kernel_size=1, padding=0),\n                nn.Sigmoid())\n\n        self.post = nn.Sequential(\n                nn.Conv2d(self.gr, 8*self.gr, kernel_size=3, padding=1))\n\n    def forward(self, x):\n        skip = x.clone()\n        att = self.att(x)\n        grad = self.grad(x)\n\n\n        se = self.se(x)\n\n        n, c, h, w = x.shape\n\n        att = att.reshape(n*self.gr, 4, h*w, 1).permute(0, 2, 1, 3)\n        grad = grad.reshape(n*self.gr, 4, h*w, 1).permute(0, 2, 1, 3)\n\n        A = self.a * att\n        B = grad * att\n\n        A = A.reshape(n*self.gr, h*w*4, h*w)\n        B = B.reshape(n*self.gr, h*w*4, 1)\n\n        AT = A.permute(0, 2, 1)\n\n        ATA = torch.bmm(AT, A)\n        ATB = torch.bmm(AT, B)\n\n        jitter = torch.eye(n=h*w, dtype=x.dtype, device=x.device).unsqueeze(0) * 1e-12\n        x, _ = torch.solve(ATB, ATA+jitter)\n\n        x = x.reshape(n, self.gr, h, w)\n\n        x = self.ins(x)\n\n        x = se * x\n\n        x = self.post(x)\n\n        return x\n\n\n\nclass Refine(nn.Module):\n    def __init__(self, c1, c2):\n        super(Refine, self).__init__()\n\n        s = c1 + c2\n        self.fw = nn.Sequential(\n                nn.Conv2d(s, s, kernel_size=3, padding=1),\n                nn.LeakyReLU(),\n                nn.Conv2d(s, c1, kernel_size=3, padding=1))\n\n        self.dw = nn.Sequential(\n                nn.Conv2d(s, s, kernel_size=3, padding=1),\n                nn.LeakyReLU(),\n                nn.Conv2d(s, c2, kernel_size=3, padding=1))\n\n    def forward(self, feat, depth):\n        cc = torch.cat([feat, depth], 1)\n        feat_new = self.fw(cc)\n        depth_new = self.dw(cc)\n        return feat_new, depth_new\n\n\nclass MetricLayer(nn.Module):\n    def __init__(self, c):\n        super(MetricLayer, self).__init__()\n\n        self.ln = nn.Sequential(\n                nn.Linear(c, c//4),\n                nn.LeakyReLU(),\n                nn.Linear(c//4, 2))\n\n    def forward(self, x):\n\n        x = x.squeeze(-1).squeeze(-1)\n        x = self.ln(x)\n        x = x.unsqueeze(-1).unsqueeze(-1)\n\n        return x\n\n\nclass VADepthNet(nn.Module):\n    def __init__(self, pretrained=None, max_depth=10.0, prior_mean=1.54, si_lambda=0.85, img_size=(480, 640)):\n        super().__init__()\n\n        self.prior_mean = prior_mean\n        self.SI_loss_lambda = si_lambda\n        self.max_depth = max_depth\n\n        pretrain_img_size = img_size\n        patch_size = (4, 4)\n        in_chans = 3\n        embed_dim = 192\n        depths = [2, 2, 18, 2]\n        num_heads = [6, 12, 24, 48]\n        window_size = 12\n\n        backbone_cfg = dict(\n            pretrain_img_size=pretrain_img_size,\n            patch_size=patch_size,\n            embed_dim=embed_dim,\n            depths=depths,\n            num_heads=num_heads,\n            window_size=window_size,\n            ape=True,\n            drop_rate=0.\n        )\n\n\n        self.backbone = SwinTransformer(**backbone_cfg)\n        \n        self.backbone.init_weights(pretrained=pretrained)\n\n        self.up_4 = Up(1536 + 768, 512)\n        self.up_3 = Up(512 + 384, 256)\n        self.up_2 = Up(256 + 192, 64)\n\n        self.outc = OutConv(128, 1, self.prior_mean)\n\n        self.vlayer = VarLayer(512, img_size[0]//16, img_size[1]//16)\n\n        self.ref_4 = Refine(512, 128)\n        self.ref_3 = Refine(256, 128)\n        self.ref_2 = Refine(64, 128)\n\n        self.var_loss = VarLoss(128, 512)\n        self.si_loss = SILogLoss(self.SI_loss_lambda, self.max_depth)\n\n        self.mlayer = nn.Sequential(\n                nn.AdaptiveMaxPool2d((1,1)),\n                MetricLayer(1536))\n\n    def forward(self, x, gts=None):\n\n        x2, x3, x4, x5 = self.backbone(x)\n\n        outs = {}\n\n        metric = self.mlayer(x5)\n\n        x = self.up_4(x5, x4)\n        \n        d = self.vlayer(x)\n\n        if self.training:\n            var_loss = self.var_loss(x, d, gts)\n\n\n        x, d  = self.ref_4(x, d)\n\n        d_u4 = F.interpolate(d, scale_factor=16, mode='bilinear', align_corners=True)\n\n        x = self.up_3(x, x3)\n\n        x, d = self.ref_3(x, F.interpolate(d, scale_factor=2, mode='bilinear', align_corners=True))\n\n        d_u3 = F.interpolate(d, scale_factor=8, mode='bilinear', align_corners=True)\n\n        x = self.up_2(x, x2)\n\n        x, d = self.ref_2(x, F.interpolate(d, scale_factor=2, mode='bilinear', align_corners=True))\n\n        d_u2 = F.interpolate(d, scale_factor=4, mode='bilinear', align_corners=True)\n\n        d = d_u2 + d_u3 + d_u4\n\n        d = torch.sigmoid(metric[:, 0:1]) * (self.outc(d) + torch.exp(metric[:, 1:2]))\n\n        outs['scale_1'] = d\n\n        if self.training:\n            si_loss = self.si_loss(outs, gts)\n            return outs['scale_1'], var_loss + si_loss\n        else:\n            return outs['scale_1']","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n/kaggle/working/VADepthNet/vadepthnet/networks/utils.py","metadata":{}},{"cell_type":"code","source":"%%writefile /kaggle/working/VADepthNet/vadepthnet/networks/utils.py\nimport warnings\nimport os\nimport os.path as osp\nimport pkgutil\nimport warnings\nfrom collections import OrderedDict\nfrom importlib import import_module\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nfrom torch.utils import model_zoo\nfrom torch.nn import functional as F\nfrom torch.nn.parallel import DataParallel, DistributedDataParallel\nfrom torch import distributed as dist\n\nTORCH_VERSION = torch.__version__\n\n\ndef resize(input,\n           size=None,\n           scale_factor=None,\n           mode='nearest',\n           align_corners=None,\n           warning=True):\n    if warning:\n        if size is not None and align_corners:\n            input_h, input_w = tuple(int(x) for x in input.shape[2:])\n            output_h, output_w = tuple(int(x) for x in size)\n            if output_h > input_h or output_w > output_h:\n                if ((output_h > 1 and output_w > 1 and input_h > 1\n                     and input_w > 1) and (output_h - 1) % (input_h - 1)\n                        and (output_w - 1) % (input_w - 1)):\n                    warnings.warn(\n                        f'When align_corners={align_corners}, '\n                        'the output would more aligned if '\n                        f'input size {(input_h, input_w)} is `x+1` and '\n                        f'out size {(output_h, output_w)} is `nx+1`')\n    if isinstance(size, torch.Size):\n        size = tuple(int(x) for x in size)\n    return F.interpolate(input, size, scale_factor, mode, align_corners)\n\n\ndef normal_init(module, mean=0, std=1, bias=0):\n    if hasattr(module, 'weight') and module.weight is not None:\n        nn.init.normal_(module.weight, mean, std)\n    if hasattr(module, 'bias') and module.bias is not None:\n        nn.init.constant_(module.bias, bias)\n\n\ndef is_module_wrapper(module):\n    module_wrappers = (DataParallel, DistributedDataParallel)\n    return isinstance(module, module_wrappers)\n\n\ndef get_dist_info():\n    if TORCH_VERSION < '1.0':\n        initialized = dist._initialized\n    else:\n        if dist.is_available():\n            initialized = dist.is_initialized()\n        else:\n            initialized = False\n    if initialized:\n        rank = dist.get_rank()\n        world_size = dist.get_world_size()\n    else:\n        rank = 0\n        world_size = 1\n    return rank, world_size\n\n\ndef load_state_dict(module, state_dict, strict=False, logger=None):\n    \"\"\"Load state_dict to a module.\n\n    This method is modified from :meth:`torch.nn.Module.load_state_dict`.\n    Default value for ``strict`` is set to ``False`` and the message for\n    param mismatch will be shown even if strict is False.\n\n    Args:\n        module (Module): Module that receives the state_dict.\n        state_dict (OrderedDict): Weights.\n        strict (bool): whether to strictly enforce that the keys\n            in :attr:`state_dict` match the keys returned by this module's\n            :meth:`~torch.nn.Module.state_dict` function. Default: ``False``.\n        logger (:obj:`logging.Logger`, optional): Logger to log the error\n            message. If not specified, print function will be used.\n    \"\"\"\n    unexpected_keys = []\n    all_missing_keys = []\n    err_msg = []\n\n    metadata = getattr(state_dict, '_metadata', None)\n    state_dict = state_dict.copy()\n    if metadata is not None:\n        state_dict._metadata = metadata\n\n    # use _load_from_state_dict to enable checkpoint version control\n    def load(module, prefix=''):\n        # recursively check parallel module in case that the model has a\n        # complicated structure, e.g., nn.Module(nn.Module(DDP))\n        if is_module_wrapper(module):\n            module = module.module\n        local_metadata = {} if metadata is None else metadata.get(\n            prefix[:-1], {})\n        module._load_from_state_dict(state_dict, prefix, local_metadata, True,\n                                     all_missing_keys, unexpected_keys,\n                                     err_msg)\n        for name, child in module._modules.items():\n            if child is not None:\n                load(child, prefix + name + '.')\n\n    load(module)\n    load = None  # break load->load reference cycle\n\n    # ignore \"num_batches_tracked\" of BN layers\n    missing_keys = [\n        key for key in all_missing_keys if 'num_batches_tracked' not in key\n    ]\n    \n    if unexpected_keys:\n        print('unexpected key in source state_dict:', \", \".join(unexpected_keys))\n    if missing_keys:\n        print('missing keys in source state_dict:', \", \".join(missing_keys))\n\n    rank, _ = get_dist_info()\n    if unexpected_keys or missing_keys:\n        print('The model and loaded state dict do not match exactly')\n\n    if strict and (unexpected_keys or missing_keys):\n        raise RuntimeError('Error in loading checkpoint')\n\n#     if unexpected_keys:\n#         err_msg.append('unexpected key in source '\n#                        f'state_dict: {\", \".join(unexpected_keys)}\\n')\n#     if missing_keys:\n#         err_msg.append(\n#             f'missing keys in source state_dict: {\", \".join(missing_keys)}\\n')\n\n#     rank, _ = get_dist_info()\n#     if len(err_msg) > 0 and rank == 0:\n#         err_msg.insert(\n#             0, 'The model and loaded state dict do not match exactly\\n')\n#         err_msg = '\\n'.join(err_msg)\n#         if strict:\n#             raise RuntimeError(err_msg)\n#         elif logger is not None:\n#             logger.warning(err_msg)\n#         else:\n#             print(err_msg)\n\n\ndef load_url_dist(url, model_dir=None):\n    \"\"\"In distributed setting, this function only download checkpoint at local\n    rank 0.\"\"\"\n    rank, world_size = get_dist_info()\n    rank = int(os.environ.get('LOCAL_RANK', rank))\n    if rank == 0:\n        checkpoint = model_zoo.load_url(url, model_dir=model_dir)\n    if world_size > 1:\n        torch.distributed.barrier()\n        if rank > 0:\n            checkpoint = model_zoo.load_url(url, model_dir=model_dir)\n    return checkpoint\n\n\ndef get_torchvision_models():\n    model_urls = dict()\n    for _, name, ispkg in pkgutil.walk_packages(torchvision.models.__path__):\n        if ispkg:\n            continue\n        _zoo = import_module(f'torchvision.models.{name}')\n        if hasattr(_zoo, 'model_urls'):\n            _urls = getattr(_zoo, 'model_urls')\n            model_urls.update(_urls)\n    return model_urls\n\n\ndef _load_checkpoint(filename, map_location=None):\n    \"\"\"Load checkpoint from somewhere (modelzoo, file, url).\n\n    Args:\n        filename (str): Accept local filepath, URL, ``torchvision://xxx``,\n            ``open-mmlab://xxx``. Please refer to ``docs/model_zoo.md`` for\n            details.\n        map_location (str | None): Same as :func:`torch.load`. Default: None.\n\n    Returns:\n        dict | OrderedDict: The loaded checkpoint. It can be either an\n            OrderedDict storing model weights or a dict containing other\n            information, which depends on the checkpoint.\n    \"\"\"\n    if filename.startswith('modelzoo://'):\n        warnings.warn('The URL scheme of \"modelzoo://\" is deprecated, please '\n                      'use \"torchvision://\" instead')\n        model_urls = get_torchvision_models()\n        model_name = filename[11:]\n        checkpoint = load_url_dist(model_urls[model_name])\n    else:\n        if not osp.isfile(filename):\n            raise IOError(f'{filename} is not a checkpoint file')\n        checkpoint = torch.load(filename, map_location=map_location)\n    return checkpoint\n\n\ndef load_checkpoint(model,\n                    filename,\n                    map_location='cpu',\n                    strict=False,\n                    logger=None):\n    \"\"\"Load checkpoint from a file or URI.\n\n    Args:\n        model (Module): Module to load checkpoint.\n        filename (str): Accept local filepath, URL, ``torchvision://xxx``,\n            ``open-mmlab://xxx``. Please refer to ``docs/model_zoo.md`` for\n            details.\n        map_location (str): Same as :func:`torch.load`.\n        strict (bool): Whether to allow different params for the model and\n            checkpoint.\n        logger (:mod:`logging.Logger` or None): The logger for error message.\n\n    Returns:\n        dict or OrderedDict: The loaded checkpoint.\n    \"\"\"\n    checkpoint = _load_checkpoint(filename, map_location)\n    # OrderedDict is a subclass of dict\n    if not isinstance(checkpoint, dict):\n        raise RuntimeError(\n            f'No state_dict found in checkpoint file {filename}')\n    # get state_dict from checkpoint\n    if 'state_dict' in checkpoint:\n        state_dict = checkpoint['state_dict']\n    elif 'model' in checkpoint:\n        state_dict = checkpoint['model']\n    else:\n        state_dict = checkpoint\n    # strip prefix of state_dict\n    if list(state_dict.keys())[0].startswith('module.'):\n        state_dict = {k[7:]: v for k, v in state_dict.items()}\n\n    # for MoBY, load model of online branch\n    if sorted(list(state_dict.keys()))[0].startswith('encoder'):\n        state_dict = {k.replace('encoder.', ''): v for k, v in state_dict.items() if k.startswith('encoder.')}\n\n    # reshape absolute position embedding\n    if state_dict.get('absolute_pos_embed') is not None:\n        absolute_pos_embed = state_dict['absolute_pos_embed']\n        N1, L, C1 = absolute_pos_embed.size()\n        N2, C2, H, W = model.absolute_pos_embed.size()\n        if N1 != N2 or C1 != C2 or L != H*W:\n#             2023年11月22日15:15:44\n#             logger.warning(\"Error in loading absolute_pos_embed, pass\")\n            print(f\"Error in loading {table_key}, pass\")\n        else:\n            state_dict['absolute_pos_embed'] = absolute_pos_embed.view(N2, H, W, C2).permute(0, 3, 1, 2)\n\n    # interpolate position bias table if needed\n    relative_position_bias_table_keys = [k for k in state_dict.keys() if \"relative_position_bias_table\" in k]\n    for table_key in relative_position_bias_table_keys:\n        table_pretrained = state_dict[table_key]\n        table_current = model.state_dict()[table_key]\n        L1, nH1 = table_pretrained.size()\n        L2, nH2 = table_current.size()\n        if nH1 != nH2:\n#             2023年11月22日15:22:02\n#             logger.warning(f\"Error in loading {table_key}, pass\")\n            print(f\"Error in loading {table_key}, pass\")\n        else:\n            if L1 != L2:\n                S1 = int(L1 ** 0.5)\n                S2 = int(L2 ** 0.5)\n                table_pretrained_resized = F.interpolate(\n                     table_pretrained.permute(1, 0).view(1, nH1, S1, S1),\n                     size=(S2, S2), mode='bicubic')\n                state_dict[table_key] = table_pretrained_resized.view(nH2, L2).permute(1, 0)\n\n    # load state_dict\n    load_state_dict(model, state_dict, strict, logger)\n#     load_state_dict(model, state_dict, strict, logger=logger)\n    return checkpoint","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# util.py原始代码\nimport warnings\nimport os\nimport os.path as osp\nimport pkgutil\nimport warnings\nfrom collections import OrderedDict\nfrom importlib import import_module\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nfrom torch.utils import model_zoo\nfrom torch.nn import functional as F\nfrom torch.nn.parallel import DataParallel, DistributedDataParallel\nfrom torch import distributed as dist\n\nTORCH_VERSION = torch.__version__\n\n\ndef resize(input,\n           size=None,\n           scale_factor=None,\n           mode='nearest',\n           align_corners=None,\n           warning=True):\n    if warning:\n        if size is not None and align_corners:\n            input_h, input_w = tuple(int(x) for x in input.shape[2:])\n            output_h, output_w = tuple(int(x) for x in size)\n            if output_h > input_h or output_w > output_h:\n                if ((output_h > 1 and output_w > 1 and input_h > 1\n                     and input_w > 1) and (output_h - 1) % (input_h - 1)\n                        and (output_w - 1) % (input_w - 1)):\n                    warnings.warn(\n                        f'When align_corners={align_corners}, '\n                        'the output would more aligned if '\n                        f'input size {(input_h, input_w)} is `x+1` and '\n                        f'out size {(output_h, output_w)} is `nx+1`')\n    if isinstance(size, torch.Size):\n        size = tuple(int(x) for x in size)\n    return F.interpolate(input, size, scale_factor, mode, align_corners)\n\n\ndef normal_init(module, mean=0, std=1, bias=0):\n    if hasattr(module, 'weight') and module.weight is not None:\n        nn.init.normal_(module.weight, mean, std)\n    if hasattr(module, 'bias') and module.bias is not None:\n        nn.init.constant_(module.bias, bias)\n\n\ndef is_module_wrapper(module):\n    module_wrappers = (DataParallel, DistributedDataParallel)\n    return isinstance(module, module_wrappers)\n\n\ndef get_dist_info():\n    if TORCH_VERSION < '1.0':\n        initialized = dist._initialized\n    else:\n        if dist.is_available():\n            initialized = dist.is_initialized()\n        else:\n            initialized = False\n    if initialized:\n        rank = dist.get_rank()\n        world_size = dist.get_world_size()\n    else:\n        rank = 0\n        world_size = 1\n    return rank, world_size\n\n\ndef load_state_dict(module, state_dict, strict=False, logger=None):\n    \"\"\"Load state_dict to a module.\n\n    This method is modified from :meth:`torch.nn.Module.load_state_dict`.\n    Default value for ``strict`` is set to ``False`` and the message for\n    param mismatch will be shown even if strict is False.\n\n    Args:\n        module (Module): Module that receives the state_dict.\n        state_dict (OrderedDict): Weights.\n        strict (bool): whether to strictly enforce that the keys\n            in :attr:`state_dict` match the keys returned by this module's\n            :meth:`~torch.nn.Module.state_dict` function. Default: ``False``.\n        logger (:obj:`logging.Logger`, optional): Logger to log the error\n            message. If not specified, print function will be used.\n    \"\"\"\n    unexpected_keys = []\n    all_missing_keys = []\n    err_msg = []\n\n    metadata = getattr(state_dict, '_metadata', None)\n    state_dict = state_dict.copy()\n    if metadata is not None:\n        state_dict._metadata = metadata\n\n    # use _load_from_state_dict to enable checkpoint version control\n    def load(module, prefix=''):\n        # recursively check parallel module in case that the model has a\n        # complicated structure, e.g., nn.Module(nn.Module(DDP))\n        if is_module_wrapper(module):\n            module = module.module\n        local_metadata = {} if metadata is None else metadata.get(\n            prefix[:-1], {})\n        module._load_from_state_dict(state_dict, prefix, local_metadata, True,\n                                     all_missing_keys, unexpected_keys,\n                                     err_msg)\n        for name, child in module._modules.items():\n            if child is not None:\n                load(child, prefix + name + '.')\n\n    load(module)\n    load = None  # break load->load reference cycle\n\n    # ignore \"num_batches_tracked\" of BN layers\n    missing_keys = [\n        key for key in all_missing_keys if 'num_batches_tracked' not in key\n    ]\n\n    if unexpected_keys:\n        err_msg.append('unexpected key in source '\n                       f'state_dict: {\", \".join(unexpected_keys)}\\n')\n    if missing_keys:\n        err_msg.append(\n            f'missing keys in source state_dict: {\", \".join(missing_keys)}\\n')\n\n    rank, _ = get_dist_info()\n    if len(err_msg) > 0 and rank == 0:\n        err_msg.insert(\n            0, 'The model and loaded state dict do not match exactly\\n')\n        err_msg = '\\n'.join(err_msg)\n        if strict:\n            raise RuntimeError(err_msg)\n        elif logger is not None:\n            logger.warning(err_msg)\n        else:\n            print(err_msg)\n\n\ndef load_url_dist(url, model_dir=None):\n    \"\"\"In distributed setting, this function only download checkpoint at local\n    rank 0.\"\"\"\n    rank, world_size = get_dist_info()\n    rank = int(os.environ.get('LOCAL_RANK', rank))\n    if rank == 0:\n        checkpoint = model_zoo.load_url(url, model_dir=model_dir)\n    if world_size > 1:\n        torch.distributed.barrier()\n        if rank > 0:\n            checkpoint = model_zoo.load_url(url, model_dir=model_dir)\n    return checkpoint\n\n\ndef get_torchvision_models():\n    model_urls = dict()\n    for _, name, ispkg in pkgutil.walk_packages(torchvision.models.__path__):\n        if ispkg:\n            continue\n        _zoo = import_module(f'torchvision.models.{name}')\n        if hasattr(_zoo, 'model_urls'):\n            _urls = getattr(_zoo, 'model_urls')\n            model_urls.update(_urls)\n    return model_urls\n\n\ndef _load_checkpoint(filename, map_location=None):\n    \"\"\"Load checkpoint from somewhere (modelzoo, file, url).\n\n    Args:\n        filename (str): Accept local filepath, URL, ``torchvision://xxx``,\n            ``open-mmlab://xxx``. Please refer to ``docs/model_zoo.md`` for\n            details.\n        map_location (str | None): Same as :func:`torch.load`. Default: None.\n\n    Returns:\n        dict | OrderedDict: The loaded checkpoint. It can be either an\n            OrderedDict storing model weights or a dict containing other\n            information, which depends on the checkpoint.\n    \"\"\"\n    if filename.startswith('modelzoo://'):\n        warnings.warn('The URL scheme of \"modelzoo://\" is deprecated, please '\n                      'use \"torchvision://\" instead')\n        model_urls = get_torchvision_models()\n        model_name = filename[11:]\n        checkpoint = load_url_dist(model_urls[model_name])\n    else:\n        if not osp.isfile(filename):\n            raise IOError(f'{filename} is not a checkpoint file')\n        checkpoint = torch.load(filename, map_location=map_location)\n    return checkpoint\n\n\ndef load_checkpoint(model,\n                    filename,\n                    map_location='cpu',\n                    strict=False,\n                    logger=None):\n    \"\"\"Load checkpoint from a file or URI.\n\n    Args:\n        model (Module): Module to load checkpoint.\n        filename (str): Accept local filepath, URL, ``torchvision://xxx``,\n            ``open-mmlab://xxx``. Please refer to ``docs/model_zoo.md`` for\n            details.\n        map_location (str): Same as :func:`torch.load`.\n        strict (bool): Whether to allow different params for the model and\n            checkpoint.\n        logger (:mod:`logging.Logger` or None): The logger for error message.\n\n    Returns:\n        dict or OrderedDict: The loaded checkpoint.\n    \"\"\"\n    checkpoint = _load_checkpoint(filename, map_location)\n    # OrderedDict is a subclass of dict\n    if not isinstance(checkpoint, dict):\n        raise RuntimeError(\n            f'No state_dict found in checkpoint file {filename}')\n    # get state_dict from checkpoint\n    if 'state_dict' in checkpoint:\n        state_dict = checkpoint['state_dict']\n    elif 'model' in checkpoint:\n        state_dict = checkpoint['model']\n    else:\n        state_dict = checkpoint\n    # strip prefix of state_dict\n    if list(state_dict.keys())[0].startswith('module.'):\n        state_dict = {k[7:]: v for k, v in state_dict.items()}\n\n    # for MoBY, load model of online branch\n    if sorted(list(state_dict.keys()))[0].startswith('encoder'):\n        state_dict = {k.replace('encoder.', ''): v for k, v in state_dict.items() if k.startswith('encoder.')}\n\n    # reshape absolute position embedding\n    if state_dict.get('absolute_pos_embed') is not None:\n        absolute_pos_embed = state_dict['absolute_pos_embed']\n        N1, L, C1 = absolute_pos_embed.size()\n        N2, C2, H, W = model.absolute_pos_embed.size()\n        if N1 != N2 or C1 != C2 or L != H*W:\n            logger.warning(\"Error in loading absolute_pos_embed, pass\")\n        else:\n            state_dict['absolute_pos_embed'] = absolute_pos_embed.view(N2, H, W, C2).permute(0, 3, 1, 2)\n\n    # interpolate position bias table if needed\n    relative_position_bias_table_keys = [k for k in state_dict.keys() if \"relative_position_bias_table\" in k]\n    for table_key in relative_position_bias_table_keys:\n        table_pretrained = state_dict[table_key]\n        table_current = model.state_dict()[table_key]\n        L1, nH1 = table_pretrained.size()\n        L2, nH2 = table_current.size()\n        if nH1 != nH2:\n            logger.warning(f\"Error in loading {table_key}, pass\")\n        else:\n            if L1 != L2:\n                S1 = int(L1 ** 0.5)\n                S2 = int(L2 ** 0.5)\n                table_pretrained_resized = F.interpolate(\n                     table_pretrained.permute(1, 0).view(1, nH1, S1, S1),\n                     size=(S2, S2), mode='bicubic')\n                state_dict[table_key] = table_pretrained_resized.view(nH2, L2).permute(1, 0)\n\n    # load state_dict\n    load_state_dict(model, state_dict, strict, logger)\n    return checkpoint","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%writefile /kaggle/working/VADepthNet/configs/arguments_train_nyu.txt\n# --mode train\n# --model_name vadepthnet\n# --pretrain /cluster/work/cvl/guosun/cr/swin_large_patch4_window12_384_22k.pth\n# --dataset nyu\n# --data_path ../dataset/nyu_depth_v2/sync/\n# --gt_path ../dataset/nyu_depth_v2/sync/\n# --filenames_file data_splits/nyudepthv2_train_files_with_gt.txt\n# --batch_size 4\n# --num_epochs 50\n# --learning_rate 3e-5\n# --end_learning_rate 1e-5\n# --weight_decay 0.0\n# --adam_eps 1e-3\n# --num_threads 1\n# --input_height 480\n# --input_width 640\n# --max_depth 10\n# --do_random_rotate\n# --degree 2.5\n# --log_directory ./models/\n# --multiprocessing_distributed\n# --dist_url tcp://127.0.0.1:2305\n\n# --log_freq 100\n# --do_online_eval\n# --eval_freq 2500\n# --data_path_eval ../dataset/nyu_depth_v2/official_splits/test/\n# --gt_path_eval ../dataset/nyu_depth_v2/official_splits/test/\n# --filenames_file_eval data_splits/nyudepthv2_test_files_with_gt.txt\n# --min_depth_eval 1e-3\n# --max_depth_eval 10\n# --eigen_crop","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"------------------------------------------------------------------------------------------------\n/kaggle/working/VADepthNet/vadepthnet/dataloaders/dataloader.py","metadata":{}},{"cell_type":"code","source":"%%writefile /kaggle/working/VADepthNet/vadepthnet/dataloaders/dataloader.py\n# /kaggle/working/VADepthNet/vadepthnet/dataloaders/dataloader.py  RS修改代码\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.utils.data.distributed\nfrom torchvision import transforms\nimport numpy as np\nfrom PIL import Image\nimport os\nimport random\nfrom utils import DistributedSamplerNoEvenlyDivisible\n# 2023年11月26日19:57:33\nimport cv2\n\n\ndef _is_pil_image(img):\n    return isinstance(img, Image.Image)\n\n\ndef _is_numpy_image(img):\n    return isinstance(img, np.ndarray) and (img.ndim in {2, 3})\n\n\ndef preprocessing_transforms(mode):\n    return transforms.Compose([\n        ToTensor(mode=mode)\n    ])\n\n\nclass NewDataLoader(object):\n    def __init__(self, args, mode):\n        if mode == 'train':\n            self.training_samples = DataLoadPreprocess(args, mode, transform=preprocessing_transforms(mode))\n            if args.distributed:\n                self.train_sampler = torch.utils.data.distributed.DistributedSampler(self.training_samples)\n            else:\n                self.train_sampler = None\n    \n            self.data = DataLoader(self.training_samples, args.batch_size,\n                                   shuffle=(self.train_sampler is None),\n                                   num_workers=args.num_threads,\n                                   pin_memory=True,\n                                   sampler=self.train_sampler)\n\n        elif mode == 'online_eval':\n            self.testing_samples = DataLoadPreprocess(args, mode, transform=preprocessing_transforms(mode))\n            if args.distributed:\n                # self.eval_sampler = torch.utils.data.distributed.DistributedSampler(self.testing_samples, shuffle=False)\n                self.eval_sampler = DistributedSamplerNoEvenlyDivisible(self.testing_samples, shuffle=False)\n            else:\n                self.eval_sampler = None\n            self.data = DataLoader(self.testing_samples, 1,\n                                   shuffle=False,\n                                   num_workers=1,\n                                   pin_memory=True,\n                                   sampler=self.eval_sampler)\n        \n        elif mode == 'test':\n            self.testing_samples = DataLoadPreprocess(args, mode, transform=preprocessing_transforms(mode))\n            self.data = DataLoader(self.testing_samples, 1, shuffle=False, num_workers=1)\n\n        else:\n            print('mode should be one of \\'train, test, online_eval\\'. Got {}'.format(mode))\n            \n            \nclass DataLoadPreprocess(Dataset):\n    def __init__(self, args, mode, transform=None, is_for_online_eval=False):\n        self.args = args\n        if mode == 'online_eval':\n            with open(args.filenames_file_eval, 'r') as f:\n                self.filenames = f.readlines()\n        else:\n            with open(args.filenames_file, 'r') as f:\n                self.filenames = f.readlines()\n    \n        self.mode = mode\n        self.transform = transform\n        self.to_tensor = ToTensor\n        self.is_for_online_eval = is_for_online_eval\n    \n    def __getitem__(self, idx):\n        sample_path = self.filenames[idx]\n        # focal = float(sample_path.split()[2])\n        focal = 518.8579\n\n        if self.mode == 'train':\n            if self.args.dataset == 'kitti':\n                rgb_file = sample_path.split()[0]\n                #depth_file = os.path.join(sample_path.split()[0].split('/')[0], sample_path.split()[1])\n                depth_file = sample_path.split()[1]\n\n                if self.args.use_right is True and random.random() > 0.5:\n                    rgb_file.replace('image_02', 'image_03')\n                    depth_file.replace('image_02', 'image_03')\n            else:\n                rgb_file = sample_path.split()[0]\n                depth_file = sample_path.split()[1]\n\n            image_path = os.path.join(self.args.data_path, \"./\" + sample_path.split()[0])\n            depth_path = os.path.join(self.args.gt_path, \"./\" + sample_path.split()[1])\n            #image_path = os.path.join(self.args.data_path, rgb_file)\n            #depth_path = os.path.join(self.args.gt_path, depth_file)\n            #print(image_path, self.args.data_path, rgb_file)\n            image = Image.open(image_path)\n            depth_gt = Image.open(depth_path)\n            \n#             if self.args.do_kb_crop is True:\n#                 height = image.height\n#                 width = image.width\n#                 top_margin = int(height - 352)\n#                 left_margin = int((width - 1216) / 2)\n#                 depth_gt = depth_gt.crop((left_margin, top_margin, left_margin + 1216, top_margin + 352))\n#                 image = image.crop((left_margin, top_margin, left_margin + 1216, top_margin + 352))\n                \n            if self.args.do_kb_crop is True:\n                height = image.height\n                width = image.width\n                top_margin = int((height - 480) / 2)\n                left_margin = int((width - 480) / 2)\n                depth_gt = depth_gt.crop((left_margin, top_margin, left_margin + 480, top_margin + 480))\n                image = image.crop((left_margin, top_margin, left_margin + 480, top_margin + 480))\n                \n            # To avoid blank boundaries due to pixel registration\n#             if self.args.dataset == 'nyu':\n#                 if self.args.input_height == 480:\n#                     depth_gt = np.array(depth_gt)\n#                     valid_mask = np.zeros_like(depth_gt)\n#                     valid_mask[45:472, 43:608] = 1\n#                     depth_gt[valid_mask==0] = 0\n#                     depth_gt = Image.fromarray(depth_gt)\n#                 else:\n#                     depth_gt = depth_gt.crop((43, 45, 608, 472))\n#                     image = image.crop((43, 45, 608, 472))\n    \n            if self.args.do_random_rotate is True:\n                random_angle = (random.random() - 0.5) * 2 * self.args.degree\n                image = self.rotate_image(image, random_angle)\n                depth_gt = self.rotate_image(depth_gt, random_angle, flag=Image.NEAREST)\n            \n            image = np.asarray(image, dtype=np.float32) / 255.0\n            depth_gt = np.asarray(depth_gt, dtype=np.float32)\n            depth_gt = np.expand_dims(depth_gt, axis=2)\n\n            if self.args.dataset == 'nyu':\n                depth_gt = depth_gt / 1000.0\n            else:\n                depth_gt = depth_gt / 256.0\n\n            if image.shape[0] != self.args.input_height or image.shape[1] != self.args.input_width:\n                image, depth_gt = self.random_crop(image, depth_gt, self.args.input_height, self.args.input_width)\n            image, depth_gt = self.train_preprocess(image, depth_gt)\n            sample = {'image': image, 'depth': depth_gt, 'focal': focal}\n        \n        else:\n            if self.mode == 'online_eval':\n                data_path = self.args.data_path_eval\n            else:\n                data_path = self.args.data_path\n\n            image_path = os.path.join(data_path, \"./\" + sample_path.split()[0])\n            image = np.asarray(Image.open(image_path), dtype=np.float32) / 255.0\n\n            if self.mode == 'online_eval':\n                gt_path = self.args.gt_path_eval\n                depth_path = os.path.join(gt_path, \"./\" + sample_path.split()[1])\n                #if self.args.dataset == 'kitti':\n                #    depth_path = os.path.join(gt_path, sample_path.split()[0].split('/')[0], sample_path.split()[1])\n                has_valid_depth = False\n                try:\n                    depth_gt = Image.open(depth_path)\n                    has_valid_depth = True\n                except IOError:\n                    depth_gt = False\n                    # print('Missing gt for {}'.format(image_path))\n\n                if has_valid_depth:\n                    depth_gt = np.asarray(depth_gt, dtype=np.float32)\n                    depth_gt = np.expand_dims(depth_gt, axis=2)\n                    if self.args.dataset == 'nyu':\n                        depth_gt = depth_gt / 1000.0\n                    else:\n                        depth_gt = depth_gt / 256.0\n\n                                                \n#             if self.args.do_kb_crop is True:\n#                 height = image.shape[0]\n#                 width = image.shape[1]\n#                 top_margin = int(height - 352)\n#                 left_margin = int((width - 1216) / 2)\n#                 image = image[top_margin:top_margin + 352, left_margin:left_margin + 1216, :]\n#                 if self.mode == 'online_eval' and has_valid_depth:\n#                     depth_gt = depth_gt[top_margin:top_margin + 352, left_margin:left_margin + 1216, :]\n            \n            if self.args.do_kb_crop is True:\n                height = image.shape[0]\n                width = image.shape[1]\n                top_margin = int((height - 480) / 2)\n                left_margin = int((width - 480) / 2)\n                image = image[top_margin:top_margin + 480, left_margin:left_margin + 480, :]\n                if self.mode == 'online_eval' and has_valid_depth:\n                    depth_gt = depth_gt[top_margin:top_margin + 480, left_margin:left_margin + 480, :]\n                        \n            if self.mode == 'online_eval':\n                sample = {'image': image, 'depth': depth_gt, 'focal': focal, 'has_valid_depth': has_valid_depth}\n            else:\n                sample = {'image': image, 'focal': focal}\n        \n        if self.transform:\n            sample = self.transform(sample)\n        \n        return sample\n    \n    def rotate_image(self, image, angle, flag=Image.BILINEAR):\n        result = image.rotate(angle, resample=flag)\n        return result\n\n    def random_crop(self, img, depth, height, width):\n#         2023年11月25日22:41:27\n        assert img.shape[0] >= height\n        assert img.shape[1] >= width\n        assert img.shape[0] == depth.shape[0]\n        assert img.shape[1] == depth.shape[1]\n        x = random.randint(0, img.shape[1] - width)\n        y = random.randint(0, img.shape[0] - height)\n        img = img[y:y + height, x:x + width, :]\n        depth = depth[y:y + height, x:x + width, :]\n        return img, depth\n    \n    \n    def train_preprocess(self, image, depth_gt):\n        # Random flipping\n        do_flip = random.random()\n        if do_flip > 0.5:\n            image = (image[:, ::-1, :]).copy()\n            depth_gt = (depth_gt[:, ::-1, :]).copy()\n#             depth_gt = depth_gt[:, ::-1].copy()\n    \n        # Random gamma, brightness, color augmentation\n        do_augment = random.random()\n        if do_augment > 0.5:\n            image = self.augment_image(image)\n    \n        return image, depth_gt\n    \n    def augment_image(self, image):\n        # gamma augmentation\n        gamma = random.uniform(0.9, 1.1)\n        image_aug = image ** gamma\n\n        # brightness augmentation\n        if self.args.dataset == 'nyu':\n            brightness = random.uniform(0.75, 1.25)\n        else:\n            brightness = random.uniform(0.9, 1.1)\n        image_aug = image_aug * brightness\n\n        # color augmentation\n        colors = np.random.uniform(0.9, 1.1, size=3)\n        white = np.ones((image.shape[0], image.shape[1]))\n        color_image = np.stack([white * colors[i] for i in range(3)], axis=2)\n        image_aug *= color_image\n        image_aug = np.clip(image_aug, 0, 1)\n\n        return image_aug\n    \n    def __len__(self):\n        return len(self.filenames)\n\n\nclass ToTensor(object):\n    def __init__(self, mode):\n        self.mode = mode\n        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    \n    def __call__(self, sample):\n        image, focal = sample['image'], sample['focal']\n        image = self.to_tensor(image)\n        image = self.normalize(image)\n\n        if self.mode == 'test':\n            return {'image': image, 'focal': focal}\n\n        depth = sample['depth']\n        if self.mode == 'train':\n            depth = self.to_tensor(depth)\n            return {'image': image, 'depth': depth, 'focal': focal}\n        else:\n            has_valid_depth = sample['has_valid_depth']\n            return {'image': image, 'depth': depth, 'focal': focal, 'has_valid_depth': has_valid_depth}\n    \n    def to_tensor(self, pic):\n        if not (_is_pil_image(pic) or _is_numpy_image(pic)):\n            raise TypeError(\n                'pic should be PIL Image or ndarray. Got {}'.format(type(pic)))\n        \n        if isinstance(pic, np.ndarray):\n            img = torch.from_numpy(pic.transpose((2, 0, 1)))\n            return img\n        \n        # handle PIL Image\n        if pic.mode == 'I':\n            img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n        elif pic.mode == 'I;16':\n            img = torch.from_numpy(np.array(pic, np.int16, copy=False))\n        else:\n            img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n        # PIL image mode: 1, L, P, I, F, RGB, YCbCr, RGBA, CMYK\n        if pic.mode == 'YCbCr':\n            nchannel = 3\n        elif pic.mode == 'I;16':\n            nchannel = 1\n        else:\n            nchannel = len(pic.mode)\n        img = img.view(pic.size[1], pic.size[0], nchannel)\n        \n        img = img.transpose(0, 1).transpose(0, 2).contiguous()\n        if isinstance(img, torch.ByteTensor):\n            return img.float()\n        else:\n            return img","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile /kaggle/working/VADepthNet/vadepthnet/dataloaders/dataloader.py\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.utils.data.distributed\nfrom torchvision import transforms\n\nimport numpy as np\nfrom PIL import Image\nimport os\nimport random\n\n# from ..utils import DistributedSamplerNoEvenlyDivisible\nfrom utils import DistributedSamplerNoEvenlyDivisible\n\n\ndef _is_pil_image(img):\n    return isinstance(img, Image.Image)\n\n\ndef _is_numpy_image(img):\n    return isinstance(img, np.ndarray) and (img.ndim in {2, 3})\n\n\ndef preprocessing_transforms(mode):\n    return transforms.Compose([\n        ToTensor(mode=mode)\n    ])\n\n\nclass NewDataLoader(object):\n    def __init__(self, args, mode):\n        if mode == 'train':\n            self.training_samples = DataLoadPreprocess(args, mode, transform=preprocessing_transforms(mode))\n            if args.distributed:\n                self.train_sampler = torch.utils.data.distributed.DistributedSampler(self.training_samples)\n            else:\n                self.train_sampler = None\n    \n            self.data = DataLoader(self.training_samples, args.batch_size,\n                                   shuffle=(self.train_sampler is None),\n                                   num_workers=args.num_threads,\n                                   pin_memory=True,\n                                   sampler=self.train_sampler)\n\n        elif mode == 'online_eval':\n            self.testing_samples = DataLoadPreprocess(args, mode, transform=preprocessing_transforms(mode))\n            if args.distributed:\n                # self.eval_sampler = torch.utils.data.distributed.DistributedSampler(self.testing_samples, shuffle=False)\n                self.eval_sampler = DistributedSamplerNoEvenlyDivisible(self.testing_samples, shuffle=False)\n            else:\n                self.eval_sampler = None\n            self.data = DataLoader(self.testing_samples, 1,\n                                   shuffle=False,\n                                   num_workers=1,\n                                   pin_memory=True,\n                                   sampler=self.eval_sampler)\n        \n        elif mode == 'test':\n            self.testing_samples = DataLoadPreprocess(args, mode, transform=preprocessing_transforms(mode))\n            self.data = DataLoader(self.testing_samples, 1, shuffle=False, num_workers=1)\n\n        else:\n            print('mode should be one of \\'train, test, online_eval\\'. Got {}'.format(mode))\n            \n            \nclass DataLoadPreprocess(Dataset):\n    def __init__(self, args, mode, transform=None, is_for_online_eval=False):\n        self.args = args\n        if mode == 'online_eval':\n            with open(args.filenames_file_eval, 'r') as f:\n                self.filenames = f.readlines()\n        else:\n            with open(args.filenames_file, 'r') as f:\n                self.filenames = f.readlines()\n    \n        self.mode = mode\n        self.transform = transform\n        self.to_tensor = ToTensor\n        self.is_for_online_eval = is_for_online_eval\n    \n    def __getitem__(self, idx):\n        sample_path = self.filenames[idx]\n        # focal = float(sample_path.split()[2])\n        focal = 518.8579\n\n        if self.mode == 'train':\n            if self.args.dataset == 'kitti':\n                rgb_file = sample_path.split()[0]\n                #depth_file = os.path.join(sample_path.split()[0].split('/')[0], sample_path.split()[1])\n                depth_file = sample_path.split()[1]\n\n                if self.args.use_right is True and random.random() > 0.5:\n                    rgb_file.replace('image_02', 'image_03')\n                    depth_file.replace('image_02', 'image_03')\n            else:\n                rgb_file = sample_path.split()[0]\n                depth_file = sample_path.split()[1]\n\n            image_path = os.path.join(self.args.data_path, \"./\" + sample_path.split()[0])\n            depth_path = os.path.join(self.args.gt_path, \"./\" + sample_path.split()[1])\n            #image_path = os.path.join(self.args.data_path, rgb_file)\n            #depth_path = os.path.join(self.args.gt_path, depth_file)\n            #print(image_path, self.args.data_path, rgb_file)\n            image = Image.open(image_path)\n            depth_gt = Image.open(depth_path)\n            \n            if self.args.do_kb_crop is True:\n                height = image.height\n                width = image.width\n                top_margin = int(height - 352)\n                left_margin = int((width - 1216) / 2)\n                depth_gt = depth_gt.crop((left_margin, top_margin, left_margin + 1216, top_margin + 352))\n                image = image.crop((left_margin, top_margin, left_margin + 1216, top_margin + 352))\n            \n            # To avoid blank boundaries due to pixel registration\n            if self.args.dataset == 'nyu':\n                if self.args.input_height == 480:\n                    depth_gt = np.array(depth_gt)\n                    valid_mask = np.zeros_like(depth_gt)\n                    valid_mask[45:472, 43:608] = 1\n                    depth_gt[valid_mask==0] = 0\n                    depth_gt = Image.fromarray(depth_gt)\n                else:\n                    depth_gt = depth_gt.crop((43, 45, 608, 472))\n                    image = image.crop((43, 45, 608, 472))\n    \n            if self.args.do_random_rotate is True:\n                random_angle = (random.random() - 0.5) * 2 * self.args.degree\n                image = self.rotate_image(image, random_angle)\n                depth_gt = self.rotate_image(depth_gt, random_angle, flag=Image.NEAREST)\n            \n            image = np.asarray(image, dtype=np.float32) / 255.0\n            depth_gt = np.asarray(depth_gt, dtype=np.float32)\n            depth_gt = np.expand_dims(depth_gt, axis=2)\n\n            if self.args.dataset == 'nyu':\n                depth_gt = depth_gt / 1000.0\n            else:\n                depth_gt = depth_gt / 256.0\n\n            if image.shape[0] != self.args.input_height or image.shape[1] != self.args.input_width:\n                image, depth_gt = self.random_crop(image, depth_gt, self.args.input_height, self.args.input_width)\n            image, depth_gt = self.train_preprocess(image, depth_gt)\n            sample = {'image': image, 'depth': depth_gt, 'focal': focal}\n        \n        else:\n            if self.mode == 'online_eval':\n                data_path = self.args.data_path_eval\n            else:\n                data_path = self.args.data_path\n\n            image_path = os.path.join(data_path, \"./\" + sample_path.split()[0])\n            image = np.asarray(Image.open(image_path), dtype=np.float32) / 255.0\n\n            if self.mode == 'online_eval':\n                gt_path = self.args.gt_path_eval\n                depth_path = os.path.join(gt_path, \"./\" + sample_path.split()[1])\n                #if self.args.dataset == 'kitti':\n                #    depth_path = os.path.join(gt_path, sample_path.split()[0].split('/')[0], sample_path.split()[1])\n                has_valid_depth = False\n                try:\n                    depth_gt = Image.open(depth_path)\n                    has_valid_depth = True\n                except IOError:\n                    depth_gt = False\n                    # print('Missing gt for {}'.format(image_path))\n\n                if has_valid_depth:\n                    depth_gt = np.asarray(depth_gt, dtype=np.float32)\n                    depth_gt = np.expand_dims(depth_gt, axis=2)\n                    if self.args.dataset == 'nyu':\n                        depth_gt = depth_gt / 1000.0\n                    else:\n                        depth_gt = depth_gt / 256.0\n\n            if self.args.do_kb_crop is True:\n                height = image.shape[0]\n                width = image.shape[1]\n                top_margin = int(height - 352)\n                left_margin = int((width - 1216) / 2)\n                image = image[top_margin:top_margin + 352, left_margin:left_margin + 1216, :]\n                if self.mode == 'online_eval' and has_valid_depth:\n                    depth_gt = depth_gt[top_margin:top_margin + 352, left_margin:left_margin + 1216, :]\n            \n            if self.mode == 'online_eval':\n                sample = {'image': image, 'depth': depth_gt, 'focal': focal, 'has_valid_depth': has_valid_depth}\n            else:\n                sample = {'image': image, 'focal': focal}\n        \n        if self.transform:\n            sample = self.transform(sample)\n        \n        return sample\n    \n    def rotate_image(self, image, angle, flag=Image.BILINEAR):\n        result = image.rotate(angle, resample=flag)\n        return result\n\n    def random_crop(self, img, depth, height, width):\n        assert img.shape[0] >= height\n        assert img.shape[1] >= width\n        assert img.shape[0] == depth.shape[0]\n        assert img.shape[1] == depth.shape[1]\n        x = random.randint(0, img.shape[1] - width)\n        y = random.randint(0, img.shape[0] - height)\n        img = img[y:y + height, x:x + width, :]\n        depth = depth[y:y + height, x:x + width, :]\n        return img, depth\n\n    def train_preprocess(self, image, depth_gt):\n        # Random flipping\n        do_flip = random.random()\n        if do_flip > 0.5:\n            image = (image[:, ::-1, :]).copy()\n            depth_gt = (depth_gt[:, ::-1, :]).copy()\n    \n        # Random gamma, brightness, color augmentation\n        do_augment = random.random()\n        if do_augment > 0.5:\n            image = self.augment_image(image)\n    \n        return image, depth_gt\n    \n    def augment_image(self, image):\n        # gamma augmentation\n        gamma = random.uniform(0.9, 1.1)\n        image_aug = image ** gamma\n\n        # brightness augmentation\n        if self.args.dataset == 'nyu':\n            brightness = random.uniform(0.75, 1.25)\n        else:\n            brightness = random.uniform(0.9, 1.1)\n        image_aug = image_aug * brightness\n\n        # color augmentation\n        colors = np.random.uniform(0.9, 1.1, size=3)\n        white = np.ones((image.shape[0], image.shape[1]))\n        color_image = np.stack([white * colors[i] for i in range(3)], axis=2)\n        image_aug *= color_image\n        image_aug = np.clip(image_aug, 0, 1)\n\n        return image_aug\n    \n    def __len__(self):\n        return len(self.filenames)\n\n\nclass ToTensor(object):\n    def __init__(self, mode):\n        self.mode = mode\n        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    \n    def __call__(self, sample):\n        image, focal = sample['image'], sample['focal']\n        image = self.to_tensor(image)\n        image = self.normalize(image)\n\n        if self.mode == 'test':\n            return {'image': image, 'focal': focal}\n\n        depth = sample['depth']\n        if self.mode == 'train':\n            depth = self.to_tensor(depth)\n            return {'image': image, 'depth': depth, 'focal': focal}\n        else:\n            has_valid_depth = sample['has_valid_depth']\n            return {'image': image, 'depth': depth, 'focal': focal, 'has_valid_depth': has_valid_depth}\n    \n    def to_tensor(self, pic):\n        if not (_is_pil_image(pic) or _is_numpy_image(pic)):\n            raise TypeError(\n                'pic should be PIL Image or ndarray. Got {}'.format(type(pic)))\n        \n        if isinstance(pic, np.ndarray):\n            img = torch.from_numpy(pic.transpose((2, 0, 1)))\n            return img\n        \n        # handle PIL Image\n        if pic.mode == 'I':\n            img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n        elif pic.mode == 'I;16':\n            img = torch.from_numpy(np.array(pic, np.int16, copy=False))\n        else:\n            img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n        # PIL image mode: 1, L, P, I, F, RGB, YCbCr, RGBA, CMYK\n        if pic.mode == 'YCbCr':\n            nchannel = 3\n        elif pic.mode == 'I;16':\n            nchannel = 1\n        else:\n            nchannel = len(pic.mode)\n        img = img.view(pic.size[1], pic.size[0], nchannel)\n        \n        img = img.transpose(0, 1).transpose(0, 2).contiguous()\n        if isinstance(img, torch.ByteTensor):\n            return img.float()\n        else:\n            return img","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# /kaggle/working/VADepthNet/vadepthnet/dataloaders/dataloader.py原始代码\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.utils.data.distributed\nfrom torchvision import transforms\n\nimport numpy as np\nfrom PIL import Image\nimport os\nimport random\n\nfrom utils import DistributedSamplerNoEvenlyDivisible\n\n\ndef _is_pil_image(img):\n    return isinstance(img, Image.Image)\n\n\ndef _is_numpy_image(img):\n    return isinstance(img, np.ndarray) and (img.ndim in {2, 3})\n\n\ndef preprocessing_transforms(mode):\n    return transforms.Compose([\n        ToTensor(mode=mode)\n    ])\n\n\nclass NewDataLoader(object):\n    def __init__(self, args, mode):\n        if mode == 'train':\n            self.training_samples = DataLoadPreprocess(args, mode, transform=preprocessing_transforms(mode))\n            if args.distributed:\n                self.train_sampler = torch.utils.data.distributed.DistributedSampler(self.training_samples)\n            else:\n                self.train_sampler = None\n    \n            self.data = DataLoader(self.training_samples, args.batch_size,\n                                   shuffle=(self.train_sampler is None),\n                                   num_workers=args.num_threads,\n                                   pin_memory=True,\n                                   sampler=self.train_sampler)\n\n        elif mode == 'online_eval':\n            self.testing_samples = DataLoadPreprocess(args, mode, transform=preprocessing_transforms(mode))\n            if args.distributed:\n                # self.eval_sampler = torch.utils.data.distributed.DistributedSampler(self.testing_samples, shuffle=False)\n                self.eval_sampler = DistributedSamplerNoEvenlyDivisible(self.testing_samples, shuffle=False)\n            else:\n                self.eval_sampler = None\n            self.data = DataLoader(self.testing_samples, 1,\n                                   shuffle=False,\n                                   num_workers=1,\n                                   pin_memory=True,\n                                   sampler=self.eval_sampler)\n        \n        elif mode == 'test':\n            self.testing_samples = DataLoadPreprocess(args, mode, transform=preprocessing_transforms(mode))\n            self.data = DataLoader(self.testing_samples, 1, shuffle=False, num_workers=1)\n\n        else:\n            print('mode should be one of \\'train, test, online_eval\\'. Got {}'.format(mode))\n            \n            \nclass DataLoadPreprocess(Dataset):\n    def __init__(self, args, mode, transform=None, is_for_online_eval=False):\n        self.args = args\n        if mode == 'online_eval':\n            with open(args.filenames_file_eval, 'r') as f:\n                self.filenames = f.readlines()\n        else:\n            with open(args.filenames_file, 'r') as f:\n                self.filenames = f.readlines()\n    \n        self.mode = mode\n        self.transform = transform\n        self.to_tensor = ToTensor\n        self.is_for_online_eval = is_for_online_eval\n    \n    def __getitem__(self, idx):\n        sample_path = self.filenames[idx]\n        # focal = float(sample_path.split()[2])\n        focal = 518.8579\n\n        if self.mode == 'train':\n            if self.args.dataset == 'kitti':\n                rgb_file = sample_path.split()[0]\n                #depth_file = os.path.join(sample_path.split()[0].split('/')[0], sample_path.split()[1])\n                depth_file = sample_path.split()[1]\n\n                if self.args.use_right is True and random.random() > 0.5:\n                    rgb_file.replace('image_02', 'image_03')\n                    depth_file.replace('image_02', 'image_03')\n            else:\n                rgb_file = sample_path.split()[0]\n                depth_file = sample_path.split()[1]\n\n            image_path = os.path.join(self.args.data_path, \"./\" + sample_path.split()[0])\n            depth_path = os.path.join(self.args.gt_path, \"./\" + sample_path.split()[1])\n            #image_path = os.path.join(self.args.data_path, rgb_file)\n            #depth_path = os.path.join(self.args.gt_path, depth_file)\n            #print(image_path, self.args.data_path, rgb_file)\n            image = Image.open(image_path)\n            depth_gt = Image.open(depth_path)\n            \n            if self.args.do_kb_crop is True:\n                height = image.height\n                width = image.width\n                top_margin = int(height - 352)\n                left_margin = int((width - 1216) / 2)\n                depth_gt = depth_gt.crop((left_margin, top_margin, left_margin + 1216, top_margin + 352))\n                image = image.crop((left_margin, top_margin, left_margin + 1216, top_margin + 352))\n            \n            # To avoid blank boundaries due to pixel registration\n            if self.args.dataset == 'nyu':\n                if self.args.input_height == 480:\n                    depth_gt = np.array(depth_gt)\n                    valid_mask = np.zeros_like(depth_gt)\n                    valid_mask[45:472, 43:608] = 1\n                    depth_gt[valid_mask==0] = 0\n                    depth_gt = Image.fromarray(depth_gt)\n                else:\n                    depth_gt = depth_gt.crop((43, 45, 608, 472))\n                    image = image.crop((43, 45, 608, 472))\n    \n            if self.args.do_random_rotate is True:\n                random_angle = (random.random() - 0.5) * 2 * self.args.degree\n                image = self.rotate_image(image, random_angle)\n                depth_gt = self.rotate_image(depth_gt, random_angle, flag=Image.NEAREST)\n            \n            image = np.asarray(image, dtype=np.float32) / 255.0\n            depth_gt = np.asarray(depth_gt, dtype=np.float32)\n            depth_gt = np.expand_dims(depth_gt, axis=2)\n\n            if self.args.dataset == 'nyu':\n                depth_gt = depth_gt / 1000.0\n            else:\n                depth_gt = depth_gt / 256.0\n\n            if image.shape[0] != self.args.input_height or image.shape[1] != self.args.input_width:\n                image, depth_gt = self.random_crop(image, depth_gt, self.args.input_height, self.args.input_width)\n            image, depth_gt = self.train_preprocess(image, depth_gt)\n            sample = {'image': image, 'depth': depth_gt, 'focal': focal}\n        \n        else:\n            if self.mode == 'online_eval':\n                data_path = self.args.data_path_eval\n            else:\n                data_path = self.args.data_path\n\n            image_path = os.path.join(data_path, \"./\" + sample_path.split()[0])\n            image = np.asarray(Image.open(image_path), dtype=np.float32) / 255.0\n\n            if self.mode == 'online_eval':\n                gt_path = self.args.gt_path_eval\n                depth_path = os.path.join(gt_path, \"./\" + sample_path.split()[1])\n                #if self.args.dataset == 'kitti':\n                #    depth_path = os.path.join(gt_path, sample_path.split()[0].split('/')[0], sample_path.split()[1])\n                has_valid_depth = False\n                try:\n                    depth_gt = Image.open(depth_path)\n                    has_valid_depth = True\n                except IOError:\n                    depth_gt = False\n                    # print('Missing gt for {}'.format(image_path))\n\n                if has_valid_depth:\n                    depth_gt = np.asarray(depth_gt, dtype=np.float32)\n                    depth_gt = np.expand_dims(depth_gt, axis=2)\n                    if self.args.dataset == 'nyu':\n                        depth_gt = depth_gt / 1000.0\n                    else:\n                        depth_gt = depth_gt / 256.0\n\n            if self.args.do_kb_crop is True:\n                height = image.shape[0]\n                width = image.shape[1]\n                top_margin = int(height - 352)\n                left_margin = int((width - 1216) / 2)\n                image = image[top_margin:top_margin + 352, left_margin:left_margin + 1216, :]\n                if self.mode == 'online_eval' and has_valid_depth:\n                    depth_gt = depth_gt[top_margin:top_margin + 352, left_margin:left_margin + 1216, :]\n            \n            if self.mode == 'online_eval':\n                sample = {'image': image, 'depth': depth_gt, 'focal': focal, 'has_valid_depth': has_valid_depth}\n            else:\n                sample = {'image': image, 'focal': focal}\n        \n        if self.transform:\n            sample = self.transform(sample)\n        \n        return sample\n    \n    def rotate_image(self, image, angle, flag=Image.BILINEAR):\n        result = image.rotate(angle, resample=flag)\n        return result\n\n    def random_crop(self, img, depth, height, width):\n        assert img.shape[0] >= height\n        assert img.shape[1] >= width\n        assert img.shape[0] == depth.shape[0]\n        assert img.shape[1] == depth.shape[1]\n        x = random.randint(0, img.shape[1] - width)\n        y = random.randint(0, img.shape[0] - height)\n        img = img[y:y + height, x:x + width, :]\n        depth = depth[y:y + height, x:x + width, :]\n        return img, depth\n\n    def train_preprocess(self, image, depth_gt):\n        # Random flipping\n        do_flip = random.random()\n        if do_flip > 0.5:\n            image = (image[:, ::-1, :]).copy()\n            depth_gt = (depth_gt[:, ::-1, :]).copy()\n    \n        # Random gamma, brightness, color augmentation\n        do_augment = random.random()\n        if do_augment > 0.5:\n            image = self.augment_image(image)\n    \n        return image, depth_gt\n    \n    def augment_image(self, image):\n        # gamma augmentation\n        gamma = random.uniform(0.9, 1.1)\n        image_aug = image ** gamma\n\n        # brightness augmentation\n        if self.args.dataset == 'nyu':\n            brightness = random.uniform(0.75, 1.25)\n        else:\n            brightness = random.uniform(0.9, 1.1)\n        image_aug = image_aug * brightness\n\n        # color augmentation\n        colors = np.random.uniform(0.9, 1.1, size=3)\n        white = np.ones((image.shape[0], image.shape[1]))\n        color_image = np.stack([white * colors[i] for i in range(3)], axis=2)\n        image_aug *= color_image\n        image_aug = np.clip(image_aug, 0, 1)\n\n        return image_aug\n    \n    def __len__(self):\n        return len(self.filenames)\n\n\nclass ToTensor(object):\n    def __init__(self, mode):\n        self.mode = mode\n        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    \n    def __call__(self, sample):\n        image, focal = sample['image'], sample['focal']\n        image = self.to_tensor(image)\n        image = self.normalize(image)\n\n        if self.mode == 'test':\n            return {'image': image, 'focal': focal}\n\n        depth = sample['depth']\n        if self.mode == 'train':\n            depth = self.to_tensor(depth)\n            return {'image': image, 'depth': depth, 'focal': focal}\n        else:\n            has_valid_depth = sample['has_valid_depth']\n            return {'image': image, 'depth': depth, 'focal': focal, 'has_valid_depth': has_valid_depth}\n    \n    def to_tensor(self, pic):\n        if not (_is_pil_image(pic) or _is_numpy_image(pic)):\n            raise TypeError(\n                'pic should be PIL Image or ndarray. Got {}'.format(type(pic)))\n        \n        if isinstance(pic, np.ndarray):\n            img = torch.from_numpy(pic.transpose((2, 0, 1)))\n            return img\n        \n        # handle PIL Image\n        if pic.mode == 'I':\n            img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n        elif pic.mode == 'I;16':\n            img = torch.from_numpy(np.array(pic, np.int16, copy=False))\n        else:\n            img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n        # PIL image mode: 1, L, P, I, F, RGB, YCbCr, RGBA, CMYK\n        if pic.mode == 'YCbCr':\n            nchannel = 3\n        elif pic.mode == 'I;16':\n            nchannel = 1\n        else:\n            nchannel = len(pic.mode)\n        img = img.view(pic.size[1], pic.size[0], nchannel)\n        \n        img = img.transpose(0, 1).transpose(0, 2).contiguous()\n        if isinstance(img, torch.ByteTensor):\n            return img.float()\n        else:\n            return img","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **替换测试和训练参数文件**","metadata":{}},{"cell_type":"markdown","source":"---\n/kaggle/working/VADepthNet/configs/arguments_train_nyu.txt","metadata":{}},{"cell_type":"code","source":"%%writefile /kaggle/working/VADepthNet/configs/arguments_train_nyu.txt\n--mode train\n--model_name vadepthnet\n--pretrain /kaggle/input/swintransfomerlargeimagenet/swin_base_patch4_window12_384_22k.pth\n--dataset nyu\n--data_path /kaggle/input/osidataset/osiDataset/osiDataset\n--gt_path /kaggle/input/osidataset/osiDataset/osiDataset\n--filenames_file /kaggle/input/smallfile/ositrain1.txt\n--batch_size 3\n--num_epochs 20\n--learning_rate 3e-5\n--end_learning_rate 1e-5\n--weight_decay 0.0\n--adam_eps 1e-3\n--num_threads 1\n--input_height 480\n--input_width 480\n--max_depth 65.6\n--do_random_rotate\n--degree 2.5\n--log_directory /kaggle/working/models/\n--multiprocessing_distributed\n--dist_url tcp://127.0.0.1:2305\n\n--log_freq 100\n--do_online_eval\n--eval_freq 13000\n--data_path_eval /kaggle/input/osidataset/osiDataset/osiDataset\n--gt_path_eval /kaggle/input/osidataset/osiDataset/osiDataset\n--filenames_file_eval /kaggle/input/smallfile/ositest1.txt\n--min_depth_eval 0\n--max_depth_eval 65.6","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"--mode train\n--model_name vadepthnet\n--pretrain /cluster/work/cvl/guosun/cr/swin_large_patch4_window12_384_22k.pth\n--dataset nyu\n--data_path ../dataset/nyu_depth_v2/sync/\n--gt_path ../dataset/nyu_depth_v2/sync/\n--filenames_file data_splits/nyudepthv2_train_files_with_gt.txt\n--batch_size 4\n--num_epochs 50\n--learning_rate 3e-5\n--end_learning_rate 1e-5\n--weight_decay 0.0\n--adam_eps 1e-3\n--num_threads 1\n--input_height 480\n--input_width 640\n--max_depth 10\n--do_random_rotate\n--degree 2.5\n--log_directory ./models/\n--multiprocessing_distributed\n--dist_url tcp://127.0.0.1:2305\n\n--log_freq 100\n--do_online_eval\n--eval_freq 2500\n--data_path_eval ../dataset/nyu_depth_v2/official_splits/test/\n--gt_path_eval ../dataset/nyu_depth_v2/official_splits/test/\n--filenames_file_eval data_splits/nyudepthv2_test_files_with_gt.txt\n--min_depth_eval 1e-3\n--max_depth_eval 10\n--eigen_crop","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n---\n/kaggle/working/VADepthNet/configs/arguments_eval_nyu.txt","metadata":{}},{"cell_type":"code","source":"%%writefile /kaggle/working/VADepthNet/configs/arguments_eval_nyu.txt\n--model_name vadepthnet\n--dataset nyu\n--input_height 480\n--input_width 640\n--max_depth 10\n--prior_mean 1.54\n--data_path_eval /kaggle/input/image-depth-estimation/data/nyu2_test\n--gt_path_eval /kaggle/input/image-depth-estimation/data/nyu2_test\n--filenames_file_eval /kaggle/working/VADepthNet/data_splits/vad_nyu2_test.txt\n--min_depth_eval 1e-3\n--max_depth_eval 10\n--eigen_crop\n--checkpoint_path /kaggle/input/depthandmodels/vadepthnet_nyu.pth","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# --model_name vadepthnet\n# --dataset nyu\n# --input_height 480\n# --input_width 640\n# --max_depth 10\n# --prior_mean 1.54\n# --data_path_eval ../dataset/nyu_depth_v2/official_splits/test/\n# --gt_path_eval ../dataset/nyu_depth_v2/official_splits/test/\n# --filenames_file_eval data_splits/nyudepthv2_test_files_with_gt.txt\n# --min_depth_eval 1e-3\n# --max_depth_eval 10\n# --eigen_crop\n\n# --checkpoint_path /cluster/work/cvl/celiuce/code/var/vadepthnet/vadepthnet_nyu.pth","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **其余功能-----------------------------------------------------------------------------**","metadata":{}},{"cell_type":"code","source":"import zipfile\nimport os\n\nsource_folder = '/kaggle/working/'\nzip_name = '/kaggle/working/vd.zip'\n\n# 检查压缩文件是否已存在，如果存在则删除以便更新\nif os.path.exists(zip_name):\n    os.remove(zip_name)\n\n# 创建一个新的压缩文件\nwith zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n    # 遍历源文件夹中的所有内容并将其添加到压缩文件中\n    for root, dirs, files in os.walk(source_folder):\n        for file in files:\n            file_path = os.path.join(root, file)\n            arcname = file_path[len(source_folder):]  # 获取相对路径\n            zipf.write(file_path, arcname)\n\nprint(f\"Files from '{source_folder}' are zipped into '{zip_name}'\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!tar -cf vad1127.tar -C /kaggle/working/ VADepthNet models result_vadepthnet ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nimport os\n\n# 源文件路径\nsource_file_path = '/kaggle/working/movemodel'\n# 目标文件夹路径\ntarget_folder_path = '/kaggle/working/model-600-best_rms_6.36519'\n\n# 检查目标文件夹是否存在，如果不存在则创建\nif not os.path.exists(target_folder_path):\n    os.makedirs(target_folder_path)\n\n# 移动文件\nshutil.move(source_file_path, target_folder_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!tar -cf frsresult.tar /kaggle/working/result_vadepthnet","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:06:11.133253Z","iopub.execute_input":"2023-11-28T16:06:11.133623Z","iopub.status.idle":"2023-11-28T16:06:12.354945Z","shell.execute_reply.started":"2023-11-28T16:06:11.133595Z","shell.execute_reply":"2023-11-28T16:06:12.353729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!tar -cf images.tar -C /kaggle/working/ *.png *.tif","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:56:57.634022Z","iopub.execute_input":"2023-11-28T15:56:57.634397Z","iopub.status.idle":"2023-11-28T15:56:58.606230Z","shell.execute_reply.started":"2023-11-28T15:56:57.634365Z","shell.execute_reply":"2023-11-28T15:56:58.604982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 文件夹压缩下载\n!tar -cf frs.tar -C /kaggle/working/ VADepthNet models result_vadepthnet \nprint(\"保存压缩包成功！\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 删除指定位置文件\nimport os\n\nfile_path = '/kaggle/working/result.tar'\n# 检查文件是否存在\nif os.path.exists(file_path):  \n    # 删除文件\n    os.remove(file_path)  \n    print(\"文件已成功删除。\")\nelse:\n    print(\"文件不存在。\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\n# 目标文件夹路径\ntarget_folder_path = '/kaggle/working/models/vadepthnet'\n\n# 检查目标文件夹是否存在\nif os.path.exists(target_folder_path):\n    # 获取目标文件夹下的所有文件列表\n    file_list = os.listdir(target_folder_path)\n\n    # 遍历文件列表\n    for file_name in file_list:\n        # 检查文件名是否以 \"model-600\" 开头\n        if file_name.startswith(\"model-600\"):\n            # 构造文件的完整路径\n            file_path = os.path.join(target_folder_path, file_name)\n\n            # 删除文件\n            os.remove(file_path)\n            print(f\"文件 {file_name} 已成功删除。\")\nelse:\n    print(\"目标文件夹不存在。\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 清空文件夹\nimport shutil\nimport os\n\nif __name__ == '__main__':\n    path = '/kaggle/working/vad1127'\n    if os.path.exists(path):\n        shutil.rmtree(path)\n        print('删除完成')\n    else:\n        print('原本为空')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 新建文件夹\nresult_folder = '/kaggle/working/models/vadepthnet'\nif not os.path.exists(result_folder):\n    os.makedirs(result_folder)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 打印文件列表及层级结构\nimport os\n\ndef print_files_with_hierarchy(folder_path, indent='', root_path=''):\n    files = os.listdir(folder_path)\n    for file in files:\n        file_path = os.path.join(folder_path, file)\n        if os.path.isdir(file_path):\n            print(indent + '📁 ' + file + ' (' + os.path.join(root_path, folder_path, file) + ')')  # 文件夹标识符\n            print_files_with_hierarchy(file_path, indent + '  ', os.path.join(root_path, folder_path))  # 递归调用以打印子文件夹的内容\n        else:\n            print(indent + '📄 ' + file + ' (' + os.path.join(root_path, folder_path, file) + ')')  # 文件标识符\n\n# 获取Output文件夹的路径\noutput_folder = '/kaggle/working'\n# 打印输出文件列表及其层级结构\nprint_files_with_hierarchy(output_folder, root_path='/kaggle/working')","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]}]}